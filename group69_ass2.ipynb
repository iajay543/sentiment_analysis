{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5149 Applied Data Analysis\n",
    "## Assignment 2: Sentiment Classification of YELP Product Reviews\n",
    "\n",
    "### Group Details:\n",
    "\n",
    "#### Group Number : 69\n",
    "#### Team Members : \n",
    "1. Rishabh Kochhar - 29869560\n",
    "2. Subhasish Sarkar - 29819253\n",
    "3. Ajay Ganapathy - 29822270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "VTYrbrKgvvWA",
    "outputId": "708da6e2-cfee-4743-fc33-eed30eb516aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlxtend\n",
    "nltk.download('all')\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from itertools import chain\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from  sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression as log_r\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zye49iavvWC"
   },
   "outputs": [],
   "source": [
    "#reading the labeled dataset\n",
    "df=pd.read_csv('labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "OUMmtesrvvWE",
    "outputId": "adffc134-6a66-4683-8200-d8b9bd348b35",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The new rule is - \\r\\nif you are waiting for a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flirted with giving this two stars, but that's...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was staying at planet Hollywood across the s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Food is good but prices are super expensive.  ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  The new rule is - \\r\\nif you are waiting for a...      4\n",
       "1  Flirted with giving this two stars, but that's...      3\n",
       "2  I was staying at planet Hollywood across the s...      5\n",
       "3  Food is good but prices are super expensive.  ...      2\n",
       "4  Worse company to deal with they do horrible wo...      1"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the top few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing labeled data and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we clean the raw text by performing several tasks as mentioned below:\n",
    "1. Normalizing the raw text\n",
    "2. Handling Contractions\n",
    "3. Removing punctuations\n",
    "4. Number Encoding\n",
    "5. Removing spaces, newline characters\n",
    "6. Tokenization\n",
    "7. Certain stopwords removal\n",
    "8. Processing Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we preprocess the labeled dataset, extract features in the labeled dataset and build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AfHEOTdvvWJ"
   },
   "outputs": [],
   "source": [
    "# Make a list of relevant POS tags since we do not care about the rest\n",
    "POS_TAGS=['CC','JJ','JJR','JJS','RBR','RBS','MD','PDT','VB','VBD','VBG','VBN',\\\n",
    "          'VBP','VBZ','WRB','WP','WDT'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dh5ZpXVbvvWN"
   },
   "outputs": [],
   "source": [
    "#function to clean the raw text where we convert the raw text to lower case, \n",
    "#expand contractions such as can't to cannot, remove punctuations, newline characters,\n",
    "#tabs, tokenize the raw text\n",
    "def CleanData(row_id,raw_text):\n",
    "    cleaned_text = list()\n",
    "    \n",
    "    # Lower case\n",
    "    proc_text=raw_text.lower()\n",
    "    \n",
    "    # Handling Contractions\n",
    "    # specific\n",
    "    proc_text = re.sub(r\"won\\'t\", \"will not\", proc_text)\n",
    "    proc_text = re.sub(r\"can\\'t\", \"can not\", proc_text)\n",
    "    # general\n",
    "    proc_text = re.sub(r\"n\\'t\", \" not\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'re\", \" are\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'s\", \" is\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'d\", \" would\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'ll\", \" will\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'t\", \" not\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'ve\", \" have\", proc_text)\n",
    "    proc_text = re.sub(r\"\\'m\", \" am\", proc_text)\n",
    "    # Handling punctuations\n",
    "    proc_text=re.sub(r'[^a-z\\s0-9]',' ',proc_text)\n",
    "    # Replacing all numbers with custom string\n",
    "    proc_text=re.sub(r'\\d+','NUM',proc_text)\n",
    "    # Handling multiple spaces\n",
    "    proc_text=re.sub(r'\\s+', ' ',proc_text, flags=re.I)\n",
    "    # Handling tabs and newline charecters\n",
    "    proc_text=re.sub(r'\\r|\\n|\\t','',proc_text)\n",
    "    # Tokenizing\n",
    "    tokens = RegexpTokenizer(r\"[\\w]{2,}\").tokenize(proc_text)\n",
    "    # Appending to new list\n",
    "    cleaned_text=tokens\n",
    "    return (row_id,cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "voY88OdYvvWR"
   },
   "outputs": [],
   "source": [
    "#removing unwanted characters such as \\r\\n\\t and preserving the emojis\n",
    "df['emojis'] = [None]*df.shape[0]\n",
    "for row_id, row in df.iterrows():\n",
    "    text = row.text\n",
    "    text = re.sub(r'\\r|\\n|\\t','',text)\n",
    "    emojis = re.findall(r'(:\\)|:\\(|:\\-\\(|:\\-\\)|:\\*|:\\-\\*|:\\/|:\\-\\/|;\\)|:D)',text)\n",
    "    text = re.sub(r'[^A-Za-z0-9]',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = text.split()\n",
    "    df['num_emojis'] = len(emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eojtLpPdvvWS"
   },
   "outputs": [],
   "source": [
    "#function to calculate the word density or average word length\n",
    "def avgWordLength(text):\n",
    "    words = text.split()\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    return np.mean(word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract several feature based numerical columns such as word counts, character counts, word density, punctuation counts, total length, capital word count, exclamation and question mark count, unique words count, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEldNm_vvvWU"
   },
   "outputs": [],
   "source": [
    "#function to extract quantitative features\n",
    "punc = ['.',',','/',':',';','-','(',')']\n",
    "\n",
    "def extract_features(df) :\n",
    "    df['word_count'] = df['text'].apply(lambda x : len(x.split()))\n",
    "    df['char_count'] = df['text'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "    df['word_density'] = df['text'].apply(avgWordLength)\n",
    "    df['punc_count'] = df['text'].apply(lambda x : len([a for a in x if a in punc]))\n",
    "    df['total_length'] = df['text'].apply(len)\n",
    "    df['capitals'] = df['text'].apply(lambda comment: sum(1 for c in comment.split() if len(c)>1 and c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['word_count']),axis=1)\n",
    "    df['num_exclamation_marks'] =df['text'].apply(lambda x: x.count('!'))\n",
    "    df['num_question_marks'] = df['text'].apply(lambda x: x.count('?'))\n",
    "    df['num_unique_words'] = df['text'].apply(lambda x: len(set(w for w in x.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['word_count']\n",
    "    print('Done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pRG_Jb8ovvWW",
    "outputId": "77eb8dad-ca7c-4e08-a8ba-cd327fa96592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# run the feature extraction function, and store the new columns generated into our original dataframe\n",
    "df=extract_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HYqO95MBvvWY",
    "outputId": "16a92e2a-f145-490d-8cdd-31cc097ee909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rkoc0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Make a list of custom stop words \n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "to_keep = ['against', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\\\n",
    "                 'doesn', \"doesn't\", 'don', \"don't\", 'few', \"further\", 'hadn', \"hadn't\", \\\n",
    "                 'has', 'hasn', \"hasn't\", \"haven\", \"haven't\", 'isn', \"isn't\", 'might',\\\n",
    "                 \"mightn't\", 'mustn', \"mustn't\", \"needn't\", \"not\", \"no\", \"nor\", \"off\",\\\n",
    "                  \"shan't\", \"should\", \"should've\", \"shouldn't\",\\\n",
    "                 \"some\", \"such\", \"under\", \"until\", \"very\", \"wasn't\", \"weren't\",\"wouldn't\",\\\n",
    "                 'ain', 'aren', 'wasn', 'weren', \"weren't\", 'won', \"won't\", 'wouldn']\n",
    "\n",
    "stop_words = [words for words in stop_words if not words in to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CkE_unFZvvWa",
    "outputId": "a56d0bb8-d7f3-4c78-c7b6-7932d4098e05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label', 'emojis', 'num_emojis', 'word_count', 'char_count',\n",
       "       'word_density', 'punc_count', 'total_length', 'capitals',\n",
       "       'caps_vs_length', 'num_exclamation_marks', 'num_question_marks',\n",
       "       'num_unique_words', 'words_vs_unique'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look into the new columns generated\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Further, we select the best features amongst the list of features on the basis of how they are related to each other. \n",
    "So, we consider features such as capital word count, word count, number of exclamation and question marks, number of unique words and unique word ratio as our best features.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lxgI0GbUvvWd",
    "outputId": "7ccb4568-dcaa-47ae-faaa-942e580b1bca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n"
     ]
    }
   ],
   "source": [
    "# making a copy of the dataframe to ensure re-usability in case of errors\n",
    "df1 = df.copy()\n",
    "print('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "SRRWFSMkvvWi",
    "outputId": "679d2880-a812-492c-9c5f-3ac16a183cbd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The new rule is - \\r\\nif you are waiting for a...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.774775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flirted with giving this two stars, but that's...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.699029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was staying at planet Hollywood across the s...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Food is good but prices are super expensive.  ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.637097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  num_emojis  \\\n",
       "0  The new rule is - \\r\\nif you are waiting for a...      4           0   \n",
       "1  Flirted with giving this two stars, but that's...      3           0   \n",
       "2  I was staying at planet Hollywood across the s...      5           0   \n",
       "3  Food is good but prices are super expensive.  ...      2           0   \n",
       "4  Worse company to deal with they do horrible wo...      1           0   \n",
       "\n",
       "   word_count  capitals  num_exclamation_marks  num_question_marks  \\\n",
       "0         111         2                      0                   1   \n",
       "1         206         0                      0                   0   \n",
       "2         111         0                      1                   0   \n",
       "3         130         0                      0                   1   \n",
       "4         124         0                      0                   0   \n",
       "\n",
       "   num_unique_words  words_vs_unique  \n",
       "0                86         0.774775  \n",
       "1               144         0.699029  \n",
       "2                79         0.711712  \n",
       "3                98         0.753846  \n",
       "4                79         0.637097  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature selection step\n",
    "#keeping the best features and dropping the rest\n",
    "drop_list = ['emojis', 'char_count', 'word_density', 'punc_count', 'total_length',\n",
    "       'caps_vs_length']\n",
    "df.drop(columns=drop_list, axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a study into the more relevant words by conducting a review into the top bigrams and relevant pos tags.\n",
    "\n",
    "We then use these to make a cleaned data column which will then be used as tfidf features in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by making a document-token pairing of all the current words, and use them to make bigrams. These bigrams are then filtered based on their importance score, which we have taken to be the **Point-wise Mutual Information** scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 800 bigrams have been taken based on these scores, and those bigrams have been POS tagged.\n",
    "\n",
    "Following this, we have kept those bigrams which have relevant POS tags, and removed the rest. \n",
    "\n",
    "These bigrams were then re-tokenized by the MWETokenizer, and then put into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oAHGckHvvWk"
   },
   "outputs": [],
   "source": [
    "# Document-token pairing by making a dictionary where the keys are the doc ids and the values are\n",
    "# the tokens in the document\n",
    "tokenized_reuters =  dict(CleanData(row_id,row['text']) for row_id,row in df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4GK92gfvvWn"
   },
   "outputs": [],
   "source": [
    "# list of all words in the dataset\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4rPUXxyvvWr"
   },
   "outputs": [],
   "source": [
    "# List of important words which do not appead in the stop words list created earlier\n",
    "words_imp = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BP9BzuPivvWu"
   },
   "outputs": [],
   "source": [
    "# finding bigrams and trigrams\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(words_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcWAfqB1vvWw"
   },
   "outputs": [],
   "source": [
    "# finding the most meaningful bigrams by using high pmi scores\n",
    "best_bigrams=bigramFinder.nbest(bigrams.pmi, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEtPeN58vvWz"
   },
   "outputs": [],
   "source": [
    "# Choose the relevant bigrams which have the POS tags we desire\n",
    "# Add them to a tagged list\n",
    "pos_tagged = list()\n",
    "for bigram in best_bigrams:\n",
    "    tagged_sentence = nltk.tag.pos_tag([bigram[0].strip('\\''),bigram[1].strip('\\'')])\n",
    "    \n",
    "    if tagged_sentence[0][1] in POS_TAGS or tagged_sentence[1][1] in POS_TAGS:\n",
    "        pos_tagged.append((tagged_sentence[0][0],tagged_sentence[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jaqoA69vvW2"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the multi-word entities\n",
    "mwetokens = MWETokenizer(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdLd0zaMvvW3"
   },
   "outputs": [],
   "source": [
    "# setting a collocations dictionary\n",
    "colloc_units = dict()\n",
    "for key, value in tokenized_reuters.items():\n",
    "    colloc_units[key] = mwetokens.tokenize(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "xeHPk1eSvvW6",
    "outputId": "c5c24270-5d09-4e8e-faf2-35de3c607dcb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['the', 'new', 'rule', 'is', 'if', 'you', 'are...</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.774775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['flirted', 'with', 'giving', 'this', 'two', '...</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.699029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['was', 'staying', 'at', 'planet', 'hollywood'...</td>\n",
       "      <td>5</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['food', 'is', 'good', 'but', 'prices', 'are',...</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['worse', 'company', 'to', 'deal', 'with', 'th...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.637097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text  label  word_count  \\\n",
       "0    0  ['the', 'new', 'rule', 'is', 'if', 'you', 'are...      4         111   \n",
       "1    1  ['flirted', 'with', 'giving', 'this', 'two', '...      3         206   \n",
       "2    2  ['was', 'staying', 'at', 'planet', 'hollywood'...      5         111   \n",
       "3    3  ['food', 'is', 'good', 'but', 'prices', 'are',...      2         130   \n",
       "4    4  ['worse', 'company', 'to', 'deal', 'with', 'th...      1         124   \n",
       "\n",
       "   capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0         2                      0                   1                86   \n",
       "1         0                      0                   0               144   \n",
       "2         0                      1                   0                79   \n",
       "3         0                      0                   1                98   \n",
       "4         0                      0                   0                79   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.774775  \n",
       "1         0.699029  \n",
       "2         0.711712  \n",
       "3         0.753846  \n",
       "4         0.637097  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now make a new dataframe which has the cleaned text, made more relevant by pos tagging and top bigram selection in it's\n",
    "# text column\n",
    "# We use the same ids and columns from the original dataframe\n",
    "\n",
    "ids = [i for i in range(0,df.shape[0])]\n",
    "df['ids'] = ids\n",
    "train_df=pd.DataFrame(colloc_units.items(), columns=['ids','text'])\n",
    "train_df['text']=train_df['text'].astype(str)\n",
    "train_df['label'] = df['label']\n",
    "train_df['word_count'] = df['word_count']\n",
    "train_df['capitals'] = df['capitals']\n",
    "train_df['num_exclamation_marks'] = df['num_exclamation_marks']\n",
    "train_df['num_question_marks'] = df['num_question_marks']\n",
    "train_df['num_unique_words'] = df['num_unique_words']\n",
    "train_df['words_vs_unique'] = df['words_vs_unique']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to de-tokenize the tokens before making TF-IDF features. Thus we have merged them so that they form coherent sentences instead of appearing as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4b-zyNDDvvW9"
   },
   "outputs": [],
   "source": [
    "#function to merge the list of tokens into strings\n",
    "def merge_tokens(x):\n",
    "    x = re.sub(r'\\[|\\]','',x)\n",
    "    x = re.sub(r'[\\'\\']','',x)\n",
    "    x = re.sub(r',','',x)\n",
    "    return x\n",
    "train_df['text'] = train_df['text'].apply(merge_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "NmBx4ZPCvvW_",
    "outputId": "e4adbb5a-e4cc-4fc7-aca0-46c2dacd2cee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the new rule is if you are waiting for table w...</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.774775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>flirted with giving this two stars but that is...</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0.699029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>was staying at planet hollywood across the str...</td>\n",
       "      <td>5</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>food is good but prices are super expensive NU...</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>0.753846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.637097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text  label  word_count  \\\n",
       "0    0  the new rule is if you are waiting for table w...      4         111   \n",
       "1    1  flirted with giving this two stars but that is...      3         206   \n",
       "2    2  was staying at planet hollywood across the str...      5         111   \n",
       "3    3  food is good but prices are super expensive NU...      2         130   \n",
       "4    4  worse company to deal with they do horrible wo...      1         124   \n",
       "\n",
       "   capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0         2                      0                   1                86   \n",
       "1         0                      0                   0               144   \n",
       "2         0                      1                   0                79   \n",
       "3         0                      0                   1                98   \n",
       "4         0                      0                   0                79   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.774775  \n",
       "1         0.699029  \n",
       "2         0.711712  \n",
       "3         0.753846  \n",
       "4         0.637097  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tzklj_I_vvXB"
   },
   "outputs": [],
   "source": [
    "# max_features is an upper bound on the number of words in the vocabulary that will be made once the vectorizer runs\n",
    "max_features = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8DEjuz0vvXC"
   },
   "outputs": [],
   "source": [
    "# train test split (80:20 ratio)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(columns=['label','ids'], axis=1), df.label, \\\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "HE-EThXsvvXE",
    "outputId": "3d1cc861-fde9-4e6c-b3bc-af08bea04b53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>have never been so out done lucille is we were...</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>0.644330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>this is my go to spot always try something new...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>would say meh but all in all it is not terribl...</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.644860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>this place has large variety of drinks and sna...</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0.735537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>horrid own business and do deposits daily have...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.792208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>this is hidden gem in king west this is my go ...</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44732</th>\n",
       "      <td>am vegas clubbing veteran would rather not dis...</td>\n",
       "      <td>279</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>0.663082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>great staff great vet had great experience wit...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>have loved this restaurant since the first tim...</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>my nail tech candy deserves NUM stars she is f...</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0.707965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "39087  have never been so out done lucille is we were...         194   \n",
       "30893  this is my go to spot always try something new...          35   \n",
       "45278  would say meh but all in all it is not terribl...         214   \n",
       "16398  this place has large variety of drinks and sna...         121   \n",
       "13653  horrid own business and do deposits daily have...          77   \n",
       "...                                                  ...         ...   \n",
       "11284  this is hidden gem in king west this is my go ...          63   \n",
       "44732  am vegas clubbing veteran would rather not dis...         279   \n",
       "38158  great staff great vet had great experience wit...          21   \n",
       "860    have loved this restaurant since the first tim...          72   \n",
       "15795  my nail tech candy deserves NUM stars she is f...         113   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "39087         0                      2                   0               125   \n",
       "30893         0                      1                   0                27   \n",
       "45278         2                      0                   0               138   \n",
       "16398         0                      0                   0                89   \n",
       "13653         3                      4                   0                61   \n",
       "...         ...                    ...                 ...               ...   \n",
       "11284         0                      3                   0                49   \n",
       "44732         3                      1                   0               185   \n",
       "38158         0                      1                   0                19   \n",
       "860           0                      1                   0                56   \n",
       "15795         0                      1                   0                80   \n",
       "\n",
       "       words_vs_unique  \n",
       "39087         0.644330  \n",
       "30893         0.771429  \n",
       "45278         0.644860  \n",
       "16398         0.735537  \n",
       "13653         0.792208  \n",
       "...                ...  \n",
       "11284         0.777778  \n",
       "44732         0.663082  \n",
       "38158         0.904762  \n",
       "860           0.777778  \n",
       "15795         0.707965  \n",
       "\n",
       "[40000 rows x 7 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at our training split\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zv3Pyy-kvvXF",
    "outputId": "f746f4b9-3161-45c5-bb33-c210932d2ba4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39087    1\n",
       "30893    5\n",
       "45278    3\n",
       "16398    3\n",
       "13653    2\n",
       "        ..\n",
       "11284    5\n",
       "44732    5\n",
       "38158    5\n",
       "860      5\n",
       "15795    2\n",
       "Name: label, Length: 40000, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at our training split labels\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYsBefmpvvXI"
   },
   "outputs": [],
   "source": [
    "# we take the numerical columns into this new list\n",
    "textcountscols = X_train.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "REHuG-C8vvXJ",
    "outputId": "55a0f8c1-7bc5-4472-b588-a1b98db7d857"
   },
   "outputs": [],
   "source": [
    "# Since we have numerical features, we need to normalise them so that there is to ambiguity in their scales\n",
    "# Furthermore after the creation of the new dataframe, some columns have a datatype of \"Object\",\n",
    "# which needs to be converted to numeric\n",
    "\n",
    "# 1. Conversion of datatype\n",
    "# 2. Min-Max Feature Scaling to scale the features\n",
    "for colname in textcountscols:\n",
    "    X_train[colname] = pd.to_numeric(X_train[colname])\n",
    "    X_train[colname] = (X_train[colname]-X_train[colname].min())/(X_train[colname].max()-X_train[colname].min())\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "TCnjUdaIvvXM",
    "outputId": "24b1feff-78aa-42fd-883d-abc13f053828"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      object\n",
       "word_count               float64\n",
       "capitals                 float64\n",
       "num_exclamation_marks    float64\n",
       "num_question_marks       float64\n",
       "num_unique_words         float64\n",
       "words_vs_unique          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for colname in textcountscols:\n",
    "    X_test[colname] = pd.to_numeric(X_test[colname])\n",
    "    X_test[colname] = (X_test[colname]-X_test[colname].min())/(X_test[colname].max()-X_test[colname].min())\n",
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K26BPA1lvvXO"
   },
   "outputs": [],
   "source": [
    "# Class for custom feature extraction pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development\n",
    "\n",
    "We have considered 3 Models for our classification task:\n",
    "\n",
    "1. Logisitc Regression\n",
    "2. Multinomial Naive Bayes\n",
    "3. Ensembled model of Logistic and Naive Bayes, and a Stacked model which consists of the ensembled model, and the base logistic and naive bayes models. \n",
    "\n",
    "**NOTE** Only the logistic model gives the highest accuracy so we have removed the code for the implementation of the other models.\n",
    "\n",
    "\n",
    "###  Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XS-TNORNvvXS"
   },
   "outputs": [],
   "source": [
    "# Building the logistic regression model by integrating custom features using FeatureUnion \n",
    "# and vectorizing those features using TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "tfidf_vect= TfidfVectorizer(use_idf=True, sublinear_tf = True, max_features = max_features, lowercase=False,\\\n",
    "                            token_pattern=r'\\w{2,}', ngram_range=(1,3), encoding='utf-8')\n",
    "\n",
    "features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols)),\\\n",
    "                           ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='text')), ('vect', tfidf_vect)]))])\n",
    "log_model = Pipeline([('features', features),\n",
    "                        ('log', OneVsRestClassifier(log_r(random_state=0, C=1.01, solver='lbfgs')))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "_XoYr6HCvvXU",
    "outputId": "1e629ed2-0d48-4f25-f01c-566e37c9be92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('textcounts',\n",
       "                                                 ColumnExtractor(cols=Index(['word_count', 'capitals', 'num_exclamation_marks', 'num_question_marks',\n",
       "       'num_unique_words', 'words_vs_unique'],\n",
       "      dtype='object'))),\n",
       "                                                ('pipe',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('cleantext',\n",
       "                                                                  ColumnExtractor(cols='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectoriz...\n",
       "                                                          verbose=False))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('log',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(C=1.01,\n",
       "                                                                  class_weight=None,\n",
       "                                                                  dual=False,\n",
       "                                                                  fit_intercept=True,\n",
       "                                                                  intercept_scaling=1,\n",
       "                                                                  l1_ratio=None,\n",
       "                                                                  max_iter=100,\n",
       "                                                                  multi_class='warn',\n",
       "                                                                  n_jobs=None,\n",
       "                                                                  penalty='l2',\n",
       "                                                                  random_state=0,\n",
       "                                                                  solver='lbfgs',\n",
       "                                                                  tol=0.0001,\n",
       "                                                                  verbose=0,\n",
       "                                                                  warm_start=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting\n",
    "log_model.fit(X_train.fillna(' '), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W1G25T7EvvXV",
    "outputId": "10738b95-08c2-4618-c3c0-a9075fc60c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "#reporting the accuracy of the model\n",
    "y_log_pred = log_model.predict(X_test.fillna(' '))\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_log_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The accuracy shown by the logistic regression model is 61% on the labelled dataset.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7_GP1Dx4TKQ"
   },
   "source": [
    "<b> The accuracy shown by the ensemble model is 60.07% on the labelled dataset <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jExq7Uz4ZHj"
   },
   "source": [
    "<b> The accuracy shown by the stacking model is 60.24% on the labelled dataset <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Out of the above 3 models, we have chosen the best model as Logistic Regression since it gives an high accuracy. </b> <br>\n",
    "We further use that model to predict labels on the unlabelled dataset as shown in the steps below.\n",
    "\n",
    "First, we pass the unlabelled dataset to the preprocessing pipeline in a similar fashion as we did it for the labelled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbfqadv_vvXp"
   },
   "source": [
    "## Pre-processing unlabeled data and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have followed the same pre-processing and feature extraction steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "lkSlw12MvvXq",
    "outputId": "948c5e43-791a-4fe3-be4a-b0812f88a431"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of our favorite places to go when it's col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The doctor was very nice, got in in a good amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nook is an immediate phoenix staple!  I ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Had a good experience when my wife and I sat a...\n",
       "1  On my first to Montreal with my gf we came her...\n",
       "2  One of our favorite places to go when it's col...\n",
       "3  The doctor was very nice, got in in a good amo...\n",
       "4  The Nook is an immediate phoenix staple!  I ca..."
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the unlabeled dataset\n",
    "unlabeled=pd.read_csv('unlabeled_data.csv')\n",
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through some visual analysis, we see that labeled dataset is a subset of unlabeled dataset. So, we drop those rows from the unlabeled dataset which exist in the labeled dataset by performing an outer join on both the datasets as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "OmsphQD54fO3",
    "outputId": "af0c0f3e-ce06-4187-ae87-32d4ac30f66e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The new rule is - \\r\\nif you are waiting for a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flirted with giving this two stars, but that's...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.699029</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  num_emojis  \\\n",
       "0  The new rule is - \\r\\nif you are waiting for a...    4.0         0.0   \n",
       "1  Flirted with giving this two stars, but that's...    3.0         0.0   \n",
       "\n",
       "   word_count  capitals  num_exclamation_marks  num_question_marks  \\\n",
       "0       111.0       2.0                    0.0                 1.0   \n",
       "1       206.0       0.0                    0.0                 0.0   \n",
       "\n",
       "   num_unique_words  words_vs_unique  ids  \n",
       "0              86.0         0.774775  0.0  \n",
       "1             144.0         0.699029  1.0  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging two datasets using outerjoin\n",
    "new_df=pd.merge(df, unlabeled, on='text', how='outer')\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HF1PhtO34fMY",
    "outputId": "2d8fc481-3cf8-4df0-9427-330d9c10a114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "605123"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of new dataframe\n",
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Ht4fpYr64fKJ",
    "outputId": "c32a3965-97dd-4634-d432-778f83b4af53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                          0\n",
       "label                    554909\n",
       "num_emojis               554909\n",
       "word_count               554909\n",
       "capitals                 554909\n",
       "num_exclamation_marks    554909\n",
       "num_question_marks       554909\n",
       "num_unique_words         554909\n",
       "words_vs_unique          554909\n",
       "ids                      554909\n",
       "dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for NaN values\n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further see that the outer join creates NaN values in the quantitative features columns and label columns. We want those rows which have NaN values as a part of our final unlabeled dataset. In this way, duplicate rows have been removed from the unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6B9kCxY94fFH",
    "outputId": "f8608667-cfe5-45cb-9acd-eea088b61f05"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>num_emojis</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>One of our favorite places to go when it's col...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>The doctor was very nice, got in in a good amo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>The Nook is an immediate phoenix staple!  I ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  num_emojis  \\\n",
       "50214  Had a good experience when my wife and I sat a...    NaN         NaN   \n",
       "50215  On my first to Montreal with my gf we came her...    NaN         NaN   \n",
       "50216  One of our favorite places to go when it's col...    NaN         NaN   \n",
       "50217  The doctor was very nice, got in in a good amo...    NaN         NaN   \n",
       "50218  The Nook is an immediate phoenix staple!  I ca...    NaN         NaN   \n",
       "\n",
       "       word_count  capitals  num_exclamation_marks  num_question_marks  \\\n",
       "50214         NaN       NaN                    NaN                 NaN   \n",
       "50215         NaN       NaN                    NaN                 NaN   \n",
       "50216         NaN       NaN                    NaN                 NaN   \n",
       "50217         NaN       NaN                    NaN                 NaN   \n",
       "50218         NaN       NaN                    NaN                 NaN   \n",
       "\n",
       "       num_unique_words  words_vs_unique  ids  \n",
       "50214               NaN              NaN  NaN  \n",
       "50215               NaN              NaN  NaN  \n",
       "50216               NaN              NaN  NaN  \n",
       "50217               NaN              NaN  NaN  \n",
       "50218               NaN              NaN  NaN  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final unlabeled dataset\n",
    "unlabeled = new_df[new_df['label'].isnull()]\n",
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove those columns having NaN and use this updated unlabeled dataset for preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sS43GG7K4fCk",
    "outputId": "cac84980-55d4-496b-9048-69242b66e0fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "unlabeled.drop(columns=['label','num_emojis','word_count','capitals','num_exclamation_marks','num_question_marks','num_unique_words','words_vs_unique','ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "9b59tlIF7AKE",
    "outputId": "80cade62-022d-4bbb-84e5-deff4b9ed582"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "50214  Had a good experience when my wife and I sat a...\n",
       "50215  On my first to Montreal with my gf we came her..."
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fIL8n_MM4e_4",
    "outputId": "9e0c03a4-dba4-4a62-9a96-af74d5f2cac1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554909"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we pass the unlabeled data to the feature extraction function where we generate some quantitative features for the textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yMAtg2QD7Z0F",
    "outputId": "ef298b44-c6d3-40fb-cd44-511c8487657d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#extracting features\n",
    "unlabeled=extract_features(unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preserve the best features and drop the irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "9fmSVEnE9htW",
    "outputId": "b43be8c6-b561-476a-da92-c730c6bcb337"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>Had a good experience when my wife and I sat a...</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>On my first to Montreal with my gf we came her...</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>0.652027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>One of our favorite places to go when it's col...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>The doctor was very nice, got in in a good amo...</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>0.692857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>The Nook is an immediate phoenix staple!  I ca...</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.639594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "50214  Had a good experience when my wife and I sat a...          90   \n",
       "50215  On my first to Montreal with my gf we came her...         296   \n",
       "50216  One of our favorite places to go when it's col...          27   \n",
       "50217  The doctor was very nice, got in in a good amo...         140   \n",
       "50218  The Nook is an immediate phoenix staple!  I ca...         197   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "50214         0                      0                   0                73   \n",
       "50215         0                      0                   0               193   \n",
       "50216         0                      1                   0                26   \n",
       "50217         2                      3                   0                97   \n",
       "50218         0                      1                   0               126   \n",
       "\n",
       "       words_vs_unique  \n",
       "50214         0.811111  \n",
       "50215         0.652027  \n",
       "50216         0.962963  \n",
       "50217         0.692857  \n",
       "50218         0.639594  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeping the  best features\n",
    "drop_list = ['char_count', 'word_density', 'punc_count', 'total_length',\n",
    "       'caps_vs_length']\n",
    "unlabeled.drop(columns=drop_list, axis = 1, inplace=True)\n",
    "unlabeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we pass the textual data to the preprocessing function where the raw reviews are being cleaned. We further remove the stopwords from the entire corpus of reviews and generate best bigrams on the basis of pointwise mutual information (pmi) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKug-Aoh9hqz"
   },
   "outputs": [],
   "source": [
    "# Document token pairing by making a dictionary where the keys are the doc ids and the values are\n",
    "# the tokens in the document\n",
    "tokenized_reuters =  dict(CleanData(row_id,row['text']) for row_id,row in unlabeled.iterrows())\n",
    "\n",
    "\n",
    "# list of all words in the dataset\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "\n",
    "# list of words not in stop words\n",
    "words_imp = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmqN-5AH9hoA"
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(words_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iBFXZuM7Ztl"
   },
   "outputs": [],
   "source": [
    "# finding the most meaningful bigrams by using high pmi scores\n",
    "best_bigrams=bigramFinder.nbest(bigrams.pmi, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIDZ_eoD7ZrU"
   },
   "outputs": [],
   "source": [
    "# Choose the relevant bigrams which have the POS tags we desire\n",
    "# Add them to a tagged list\n",
    "pos_tagged = list()\n",
    "for bigram in best_bigrams:\n",
    "    tagged_sentence = nltk.tag.pos_tag([bigram[0].strip('\\''),bigram[1].strip('\\'')])\n",
    "    \n",
    "    if tagged_sentence[0][1] in POS_TAGS or tagged_sentence[1][1] in POS_TAGS:\n",
    "        pos_tagged.append((tagged_sentence[0][0],tagged_sentence[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we tokenize the multi-word bigrams using MWETokenizer as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AYOxdjLwvvX4"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the multi-word entities\n",
    "mwetokens = MWETokenizer(pos_tagged)\n",
    "\n",
    "colloc_units = dict()\n",
    "for key, value in tokenized_reuters.items():\n",
    "    colloc_units[key] = mwetokens.tokenize(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ids = [i for i in range(0,unlabeled.shape[0])]\n",
    "unlabeled['ids'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting data from dictionary into a dataframe\n",
    "train_df_unlab=pd.DataFrame(colloc_units.items(), columns=['ids','text'])\n",
    "train_df_unlab['text']=train_df_unlab['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50214</td>\n",
       "      <td>['had', 'good', 'experience', 'when', 'my', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50215</td>\n",
       "      <td>['on', 'my', 'first', 'to', 'montreal', 'with'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ids                                               text\n",
       "0  50214  ['had', 'good', 'experience', 'when', 'my', 'w...\n",
       "1  50215  ['on', 'my', 'first', 'to', 'montreal', 'with'..."
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unlab.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_unlab.set_index('ids', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the quantitative features from unlabeled dataset to new dataset\n",
    "train_df_unlab['word_count'] = unlabeled['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Msfw2Bd-vvX5",
    "outputId": "5040cf2a-0e09-4f8d-991f-75dd1644c340"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>['had', 'good', 'experience', 'when', 'my', 'w...</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>['on', 'my', 'first', 'to', 'montreal', 'with'...</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>0.652027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>['one', 'of', 'our', 'favorite', 'places', 'to...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>['the', 'doctor', 'was', 'very', 'nice', 'got'...</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>0.692857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>['the', 'nook', 'is', 'an', 'immediate', 'phoe...</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.639594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50214  ['had', 'good', 'experience', 'when', 'my', 'w...          90   \n",
       "50215  ['on', 'my', 'first', 'to', 'montreal', 'with'...         296   \n",
       "50216  ['one', 'of', 'our', 'favorite', 'places', 'to...          27   \n",
       "50217  ['the', 'doctor', 'was', 'very', 'nice', 'got'...         140   \n",
       "50218  ['the', 'nook', 'is', 'an', 'immediate', 'phoe...         197   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50214         0                      0                   0                73   \n",
       "50215         0                      0                   0               193   \n",
       "50216         0                      1                   0                26   \n",
       "50217         2                      3                   0                97   \n",
       "50218         0                      1                   0               126   \n",
       "\n",
       "       words_vs_unique  \n",
       "ids                     \n",
       "50214         0.811111  \n",
       "50215         0.652027  \n",
       "50216         0.962963  \n",
       "50217         0.692857  \n",
       "50218         0.639594  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unlab['capitals'] = unlabeled['capitals']\n",
    "train_df_unlab['num_exclamation_marks'] = unlabeled['num_exclamation_marks']\n",
    "train_df_unlab['num_question_marks'] = unlabeled['num_question_marks']\n",
    "train_df_unlab['num_unique_words'] = unlabeled['num_unique_words']\n",
    "train_df_unlab['words_vs_unique'] = unlabeled['words_vs_unique']\n",
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cI9PujpzvvX7"
   },
   "outputs": [],
   "source": [
    "#merging the tokens into strings\n",
    "train_df_unlab['text'] = train_df_unlab['text'].apply(merge_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>had good experience when my wife and sat at th...</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "      <td>296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>0.652027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>one of our favorite places to go when it is co...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>the doctor was very nice got in in good amount...</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>0.692857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>the nook is an immediate phoenix staple came h...</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.639594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50214  had good experience when my wife and sat at th...          90   \n",
       "50215  on my first to montreal with my gf we came her...         296   \n",
       "50216  one of our favorite places to go when it is co...          27   \n",
       "50217  the doctor was very nice got in in good amount...         140   \n",
       "50218  the nook is an immediate phoenix staple came h...         197   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50214         0                      0                   0                73   \n",
       "50215         0                      0                   0               193   \n",
       "50216         0                      1                   0                26   \n",
       "50217         2                      3                   0                97   \n",
       "50218         0                      1                   0               126   \n",
       "\n",
       "       words_vs_unique  \n",
       "ids                     \n",
       "50214         0.811111  \n",
       "50215         0.652027  \n",
       "50216         0.962963  \n",
       "50217         0.692857  \n",
       "50218         0.639594  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "QT4pkKq2vvX8",
    "outputId": "cc6fb207-0362-4d16-ca9b-bbdc541337dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      object\n",
       "word_count               float64\n",
       "capitals                 float64\n",
       "num_exclamation_marks    float64\n",
       "num_question_marks       float64\n",
       "num_unique_words         float64\n",
       "words_vs_unique          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Min-Max Normalization of the extracted quantitative features.\n",
    "for colname in textcountscols:\n",
    "    train_df_unlab[colname] = pd.to_numeric(train_df_unlab[colname])\n",
    "    train_df_unlab[colname] = (train_df_unlab[colname]-train_df_unlab[colname].min())/(train_df_unlab[colname].max()-\\\n",
    "                                                                                          train_df_unlab[colname].min())\n",
    "train_df_unlab.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the logistic model to predict the labels on the unlabelled dataset as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMu4bpeMvvYA"
   },
   "outputs": [],
   "source": [
    "# Get the class probabilites\n",
    "# predict_proba gives a vector of values which correspond to the probability of that observation belonging to each class\n",
    "y_train_unlab_proba = log_model.predict_proba(train_df_unlab.fillna(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIstAOGWvvYB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.30296795, 0.13528451, 0.28217507, 0.18408618, 0.0954863 ]),\n",
       " array([0.00619308, 0.12128443, 0.33860192, 0.36274745, 0.17117312]),\n",
       " array([0.03860827, 0.08587971, 0.08760938, 0.16113237, 0.62677026]),\n",
       " array([0.58803309, 0.17142658, 0.0997028 , 0.06039315, 0.08044438]),\n",
       " array([0.0073796 , 0.01551592, 0.12903339, 0.14341813, 0.70465297]),\n",
       " array([0.25572927, 0.20696401, 0.07486961, 0.20477561, 0.25766151]),\n",
       " array([0.02791756, 0.08525593, 0.08121778, 0.43761734, 0.36799139]),\n",
       " array([0.1794823 , 0.59406279, 0.19623708, 0.02262795, 0.00758988]),\n",
       " array([0.01160255, 0.03508851, 0.4207284 , 0.37811107, 0.15446948]),\n",
       " array([0.01453531, 0.1920774 , 0.32295283, 0.34370752, 0.12672694]),\n",
       " array([0.25467369, 0.34815149, 0.18434181, 0.07449309, 0.13833992]),\n",
       " array([0.1273478 , 0.14169209, 0.06688821, 0.10035799, 0.56371391]),\n",
       " array([0.14260298, 0.27113736, 0.33286977, 0.19499746, 0.05839243]),\n",
       " array([0.00360046, 0.00529366, 0.00965459, 0.26627194, 0.71517935]),\n",
       " array([0.0244877 , 0.02700747, 0.06922786, 0.27777551, 0.60150146]),\n",
       " array([0.00976865, 0.06140458, 0.1912664 , 0.50593844, 0.23162193]),\n",
       " array([0.40765525, 0.16088896, 0.13334869, 0.16106781, 0.13703928]),\n",
       " array([0.72218541, 0.11499544, 0.03298236, 0.03489401, 0.09494278]),\n",
       " array([0.28242547, 0.26215027, 0.31144595, 0.12164638, 0.02233193]),\n",
       " array([0.01472385, 0.0776258 , 0.14711313, 0.63008327, 0.13045394]),\n",
       " array([0.01317307, 0.06840656, 0.17134484, 0.38448179, 0.36259375]),\n",
       " array([0.00545497, 0.04931896, 0.30935664, 0.60196556, 0.03390387]),\n",
       " array([0.67309305, 0.28428869, 0.01682183, 0.00784723, 0.0179492 ]),\n",
       " array([0.29493524, 0.50481011, 0.15054597, 0.032958  , 0.01675068]),\n",
       " array([0.00510005, 0.09947222, 0.51060342, 0.32804734, 0.05677697]),\n",
       " array([0.02073579, 0.10924442, 0.36694926, 0.47635153, 0.026719  ]),\n",
       " array([0.10122949, 0.55331425, 0.2553441 , 0.06476499, 0.02534718]),\n",
       " array([0.01264148, 0.06951539, 0.27033402, 0.48603716, 0.16147195]),\n",
       " array([0.04944361, 0.06895106, 0.09003622, 0.47792228, 0.31364684]),\n",
       " array([0.03781779, 0.05949399, 0.02893674, 0.23522615, 0.63852533]),\n",
       " array([0.03749284, 0.08768086, 0.31393289, 0.44982024, 0.11107316]),\n",
       " array([0.79943632, 0.10621236, 0.02819286, 0.02503036, 0.0411281 ]),\n",
       " array([0.00852998, 0.01539421, 0.04703595, 0.33192905, 0.59711081]),\n",
       " array([0.25755447, 0.06400603, 0.03516187, 0.08597644, 0.55730119]),\n",
       " array([0.11927062, 0.22497365, 0.26849282, 0.25222109, 0.13504182]),\n",
       " array([0.02832979, 0.04766378, 0.14169139, 0.20479716, 0.57751788]),\n",
       " array([0.59157878, 0.31064352, 0.03592023, 0.03890933, 0.02294814]),\n",
       " array([0.37330302, 0.38915266, 0.12066728, 0.06921188, 0.04766517]),\n",
       " array([0.00498674, 0.09950693, 0.62101353, 0.24435697, 0.03013583]),\n",
       " array([0.01885848, 0.03665397, 0.04047908, 0.27243024, 0.63157823]),\n",
       " array([0.03259617, 0.15585447, 0.2682671 , 0.4020434 , 0.14123887]),\n",
       " array([0.09726228, 0.17852868, 0.23557668, 0.37908611, 0.10954625]),\n",
       " array([0.08413453, 0.20802028, 0.44967793, 0.19735226, 0.060815  ]),\n",
       " array([0.08531822, 0.39218026, 0.41771629, 0.06483969, 0.03994554]),\n",
       " array([0.60752883, 0.1199592 , 0.1265874 , 0.05997668, 0.08594788]),\n",
       " array([0.02316707, 0.12306339, 0.30568033, 0.47489399, 0.07319522]),\n",
       " array([0.19915912, 0.18393079, 0.11740168, 0.32992126, 0.16958714]),\n",
       " array([0.06963404, 0.19046931, 0.43393488, 0.20998597, 0.0959758 ]),\n",
       " array([0.46325554, 0.25131198, 0.05439548, 0.09456811, 0.1364689 ]),\n",
       " array([0.46325554, 0.25131198, 0.05439548, 0.09456811, 0.1364689 ]),\n",
       " array([0.08025009, 0.15064319, 0.22985822, 0.51291581, 0.02633269]),\n",
       " array([0.04003306, 0.09849395, 0.40064982, 0.3462793 , 0.11454387]),\n",
       " array([0.31044534, 0.44901394, 0.14897821, 0.04685335, 0.04470916]),\n",
       " array([0.01366477, 0.03905532, 0.14660819, 0.55060575, 0.25006597]),\n",
       " array([0.00234384, 0.04861409, 0.28453965, 0.55303046, 0.11147195]),\n",
       " array([0.06426491, 0.1091045 , 0.27374245, 0.50803984, 0.04484829]),\n",
       " array([0.06394702, 0.08648291, 0.06385584, 0.41496405, 0.37075018]),\n",
       " array([0.04721738, 0.16971522, 0.53235645, 0.19507914, 0.05563181]),\n",
       " array([0.01644301, 0.16261367, 0.45953207, 0.34688537, 0.01452589]),\n",
       " array([0.71336754, 0.12923809, 0.05290841, 0.02865622, 0.07582975]),\n",
       " array([0.01518   , 0.08288793, 0.14900927, 0.54340051, 0.20952229]),\n",
       " array([0.44169632, 0.2020101 , 0.16976293, 0.0595252 , 0.12700545]),\n",
       " array([0.4031255 , 0.37208878, 0.08986365, 0.05871471, 0.07620736]),\n",
       " array([0.15048565, 0.44710146, 0.20921835, 0.09623127, 0.09696327]),\n",
       " array([0.00433877, 0.01704773, 0.04592111, 0.5031879 , 0.42950448]),\n",
       " array([0.31234846, 0.36913505, 0.14881146, 0.06040765, 0.10929739]),\n",
       " array([0.06063758, 0.1157502 , 0.35413493, 0.3410042 , 0.12847309]),\n",
       " array([0.01594818, 0.04022522, 0.06563557, 0.22173429, 0.65645675]),\n",
       " array([0.03821679, 0.08541282, 0.10435114, 0.46063839, 0.31138086]),\n",
       " array([0.03356492, 0.27699981, 0.5176025 , 0.15703819, 0.01479459]),\n",
       " array([0.00769722, 0.11498674, 0.37559417, 0.44579976, 0.05592212]),\n",
       " array([0.01243652, 0.23782164, 0.44093488, 0.25607005, 0.05273691]),\n",
       " array([0.04866712, 0.15971952, 0.38604782, 0.20985496, 0.19571057]),\n",
       " array([0.05508678, 0.123967  , 0.36296305, 0.38984832, 0.06813485]),\n",
       " array([0.43083154, 0.06765475, 0.03366672, 0.0447926 , 0.42305439]),\n",
       " array([0.00861758, 0.00608097, 0.03584342, 0.4639633 , 0.48549472]),\n",
       " array([0.00212355, 0.002551  , 0.07051354, 0.43208718, 0.49272473]),\n",
       " array([0.31264545, 0.37027324, 0.2446109 , 0.02774972, 0.04472069]),\n",
       " array([0.24462263, 0.35913632, 0.30868765, 0.04123557, 0.04631784]),\n",
       " array([0.85994108, 0.08807132, 0.03563399, 0.00692393, 0.00942967]),\n",
       " array([0.00339834, 0.10052248, 0.45106857, 0.38149368, 0.06351693]),\n",
       " array([0.05076358, 0.03852944, 0.09016874, 0.19384692, 0.62669131]),\n",
       " array([0.69019029, 0.12482673, 0.04055143, 0.03650001, 0.10793154]),\n",
       " array([0.08346537, 0.10028179, 0.23280416, 0.42704983, 0.15639885]),\n",
       " array([0.40701562, 0.1937849 , 0.11793995, 0.25241518, 0.02884434]),\n",
       " array([0.11481848, 0.15108273, 0.26549508, 0.37619518, 0.09240853]),\n",
       " array([0.00641873, 0.05506988, 0.45354081, 0.32694675, 0.15802383]),\n",
       " array([0.01910204, 0.06838126, 0.02351118, 0.44377931, 0.44522621]),\n",
       " array([0.06299872, 0.09739817, 0.18859008, 0.30470849, 0.34630453]),\n",
       " array([0.00424911, 0.06754351, 0.22545685, 0.57988135, 0.12286918]),\n",
       " array([0.00924506, 0.21442017, 0.42358078, 0.29105344, 0.06170054]),\n",
       " array([0.00766299, 0.01489073, 0.06856841, 0.17256039, 0.73631748]),\n",
       " array([0.00603334, 0.01997331, 0.04812991, 0.45295175, 0.47291169]),\n",
       " array([0.50949133, 0.13307665, 0.08252298, 0.05636138, 0.21854767]),\n",
       " array([0.02271138, 0.01944055, 0.0461497 , 0.24670965, 0.66498872]),\n",
       " array([0.05854158, 0.1373442 , 0.25464498, 0.44086973, 0.10859951]),\n",
       " array([0.01715682, 0.15129571, 0.15596183, 0.36021873, 0.3153669 ]),\n",
       " array([0.46321955, 0.38828942, 0.08714578, 0.02362738, 0.03771787]),\n",
       " array([0.59126285, 0.26221434, 0.05540978, 0.02840119, 0.06271184]),\n",
       " array([0.05428993, 0.21594268, 0.64231226, 0.07094703, 0.0165081 ]),\n",
       " array([0.65823389, 0.19364241, 0.09548279, 0.03064543, 0.02199548]),\n",
       " array([0.09529361, 0.07540634, 0.21803664, 0.22215293, 0.38911048]),\n",
       " array([0.01839522, 0.16303196, 0.3004252 , 0.37990973, 0.1382379 ]),\n",
       " array([0.00521072, 0.11447305, 0.26790398, 0.57558921, 0.03682304]),\n",
       " array([0.46720144, 0.36187841, 0.0956517 , 0.06777684, 0.00749161]),\n",
       " array([0.2136433 , 0.18810003, 0.13653959, 0.34447871, 0.11723836]),\n",
       " array([0.29169045, 0.1453994 , 0.0890481 , 0.27991721, 0.19394484]),\n",
       " array([0.0222256 , 0.04679123, 0.20888199, 0.41915007, 0.30295112]),\n",
       " array([0.24305081, 0.42228843, 0.11052173, 0.08509208, 0.13904696]),\n",
       " array([0.18641603, 0.64527744, 0.08588161, 0.0460994 , 0.03632551]),\n",
       " array([0.00716016, 0.11704334, 0.44055991, 0.40226887, 0.03296771]),\n",
       " array([0.45597789, 0.28041378, 0.10454417, 0.13507059, 0.02399357]),\n",
       " array([0.00736416, 0.0183236 , 0.03876507, 0.12174985, 0.81379732]),\n",
       " array([0.01506298, 0.07995847, 0.40999003, 0.40213907, 0.09284945]),\n",
       " array([0.11729496, 0.54960628, 0.17027796, 0.08077186, 0.08204894]),\n",
       " array([0.09089994, 0.16066687, 0.16707269, 0.27159709, 0.30976341]),\n",
       " array([0.32276753, 0.46891768, 0.16338717, 0.02734788, 0.01757973]),\n",
       " array([0.09834189, 0.56360398, 0.26823697, 0.06491584, 0.00490131]),\n",
       " array([0.66574808, 0.26473162, 0.03136636, 0.01706276, 0.02109118]),\n",
       " array([0.04509129, 0.3287264 , 0.40033916, 0.15373435, 0.0721088 ]),\n",
       " array([0.0441626 , 0.34066753, 0.37436351, 0.10617704, 0.13462932]),\n",
       " array([0.0613126 , 0.11849486, 0.28782591, 0.33436249, 0.19800414]),\n",
       " array([0.1809659 , 0.08586051, 0.11827873, 0.14770492, 0.46718993]),\n",
       " array([0.05649882, 0.15221796, 0.23661906, 0.34574824, 0.20891592]),\n",
       " array([0.060919  , 0.42964717, 0.25894438, 0.21441187, 0.03607758]),\n",
       " array([0.27179296, 0.15953963, 0.13256563, 0.19800875, 0.23809302]),\n",
       " array([0.05010813, 0.25693636, 0.42718719, 0.21800914, 0.04775918]),\n",
       " array([0.00358797, 0.01036066, 0.01709338, 0.27384429, 0.6951137 ]),\n",
       " array([0.45584629, 0.37932297, 0.14006956, 0.01566438, 0.0090968 ]),\n",
       " array([0.14863112, 0.26789291, 0.2656998 , 0.23559784, 0.08217833]),\n",
       " array([0.04108186, 0.23162708, 0.46509374, 0.18392915, 0.07826818]),\n",
       " array([0.23927051, 0.17896511, 0.44184726, 0.08607711, 0.05384   ]),\n",
       " array([0.04452597, 0.02159715, 0.19439693, 0.22479752, 0.51468243]),\n",
       " array([0.01536561, 0.01682484, 0.08276257, 0.47762679, 0.4074202 ]),\n",
       " array([0.00761733, 0.04360733, 0.26777278, 0.45774822, 0.22325434]),\n",
       " array([0.05918429, 0.40361317, 0.42725774, 0.08067346, 0.02927134]),\n",
       " array([0.39389045, 0.38507577, 0.15488565, 0.05448819, 0.01165994]),\n",
       " array([0.02316569, 0.04181676, 0.04068021, 0.19659404, 0.6977433 ]),\n",
       " array([0.12793906, 0.29237899, 0.28646984, 0.17126869, 0.12194342]),\n",
       " array([0.03170948, 0.02229342, 0.03861704, 0.15990501, 0.74747505]),\n",
       " array([0.85260264, 0.07957033, 0.03759989, 0.01841017, 0.01181696]),\n",
       " array([0.28263038, 0.2431847 , 0.33408083, 0.10290236, 0.03720172]),\n",
       " array([0.60383981, 0.1861219 , 0.03768022, 0.05186217, 0.1204959 ]),\n",
       " array([0.34427596, 0.27879919, 0.2084324 , 0.14235965, 0.0261328 ]),\n",
       " array([0.48184867, 0.19089792, 0.22018901, 0.03413367, 0.07293073]),\n",
       " array([0.02682037, 0.18225463, 0.41374181, 0.28717074, 0.09001245]),\n",
       " array([0.50083445, 0.26572423, 0.18194114, 0.03613325, 0.01536694]),\n",
       " array([0.53932379, 0.25318939, 0.08094574, 0.07227227, 0.05426881]),\n",
       " array([0.70979993, 0.26243714, 0.01373706, 0.00461795, 0.00940792]),\n",
       " array([0.00772279, 0.02903529, 0.08945244, 0.28789817, 0.58589131]),\n",
       " array([0.01251243, 0.0174977 , 0.01303915, 0.23318828, 0.72376245]),\n",
       " array([0.01845589, 0.02498482, 0.06701784, 0.22559704, 0.66394441]),\n",
       " array([0.24982892, 0.13277506, 0.07263655, 0.1625902 , 0.38216926]),\n",
       " array([0.10139229, 0.08012944, 0.12109796, 0.14716321, 0.5502171 ]),\n",
       " array([0.04198368, 0.14560804, 0.14230597, 0.29407107, 0.37603124]),\n",
       " array([0.00147208, 0.00418377, 0.03925643, 0.28826859, 0.66681913]),\n",
       " array([0.07287702, 0.18118558, 0.37361793, 0.12757939, 0.24474009]),\n",
       " array([0.79092079, 0.08772635, 0.0470823 , 0.02367677, 0.05059378]),\n",
       " array([0.05952478, 0.03994794, 0.08222176, 0.09501599, 0.72328954]),\n",
       " array([0.23596102, 0.49321834, 0.15914438, 0.08254375, 0.02913251]),\n",
       " array([0.05876258, 0.22955544, 0.42098758, 0.17822228, 0.11247211]),\n",
       " array([0.08934175, 0.2339768 , 0.4905318 , 0.15002888, 0.03612077]),\n",
       " array([0.38845241, 0.26145661, 0.29701957, 0.04302607, 0.01004533]),\n",
       " array([0.009986  , 0.45605287, 0.45383977, 0.06996069, 0.01016068]),\n",
       " array([0.13366069, 0.16657699, 0.19574474, 0.35835002, 0.14566755]),\n",
       " array([0.22575032, 0.44316189, 0.25117358, 0.07242002, 0.0074942 ]),\n",
       " array([0.00670562, 0.01976575, 0.17738776, 0.49150446, 0.30463641]),\n",
       " array([0.00330084, 0.00785506, 0.04457018, 0.22287875, 0.72139517]),\n",
       " array([0.76109084, 0.13813254, 0.03631765, 0.04074371, 0.02371526]),\n",
       " array([0.00187843, 0.10901405, 0.5198585 , 0.34612308, 0.02312594]),\n",
       " array([0.00864397, 0.29641281, 0.32963609, 0.33445505, 0.03085208]),\n",
       " array([0.12032626, 0.09029541, 0.26737986, 0.31304603, 0.20895244]),\n",
       " array([0.11270539, 0.11655286, 0.090378  , 0.17337729, 0.50698647]),\n",
       " array([0.06444814, 0.45445268, 0.43904428, 0.03431489, 0.00774001]),\n",
       " array([0.35850754, 0.4129335 , 0.17808934, 0.04104801, 0.00942162]),\n",
       " array([0.81474989, 0.06730999, 0.05183815, 0.02160087, 0.04450108]),\n",
       " array([0.04213743, 0.07073847, 0.42319066, 0.43670764, 0.0272258 ]),\n",
       " array([0.09658529, 0.06631206, 0.11348145, 0.15674951, 0.56687169]),\n",
       " array([0.13846863, 0.16797747, 0.12780037, 0.36692028, 0.19883325]),\n",
       " array([0.04748358, 0.05639155, 0.11172768, 0.1730093 , 0.61138789]),\n",
       " array([0.69526125, 0.21572585, 0.02588296, 0.01427458, 0.04885537]),\n",
       " array([0.03669346, 0.47145266, 0.41600411, 0.068789  , 0.00706077]),\n",
       " array([0.06320472, 0.10628876, 0.40505701, 0.36582872, 0.05962079]),\n",
       " array([0.11045511, 0.2613219 , 0.4032148 , 0.18164404, 0.04336414]),\n",
       " array([0.0834787 , 0.17257972, 0.40964387, 0.30575832, 0.0285394 ]),\n",
       " array([0.0431253 , 0.30433712, 0.22993427, 0.41136703, 0.01123628]),\n",
       " array([0.31869401, 0.49587825, 0.07990608, 0.07295807, 0.03256359]),\n",
       " array([0.06310958, 0.02441623, 0.02482445, 0.23099125, 0.65665848]),\n",
       " array([0.77471547, 0.14057651, 0.03026756, 0.02227958, 0.03216087]),\n",
       " array([0.02765157, 0.02986001, 0.08488133, 0.33736289, 0.5202442 ]),\n",
       " array([0.15818938, 0.19975716, 0.18489576, 0.20566476, 0.25149294]),\n",
       " array([0.53481315, 0.22168966, 0.1430832 , 0.0428146 , 0.05759939]),\n",
       " array([0.70185731, 0.20117714, 0.0397271 , 0.02739372, 0.02984473]),\n",
       " array([0.00783735, 0.02515157, 0.26497886, 0.56539435, 0.13663787]),\n",
       " array([0.14059675, 0.51960799, 0.28338994, 0.04996389, 0.00644143]),\n",
       " array([0.17649757, 0.24913122, 0.25852706, 0.14021343, 0.17563072]),\n",
       " array([0.00641528, 0.03453136, 0.33546252, 0.56102462, 0.06256623]),\n",
       " array([0.17559053, 0.4381123 , 0.24375274, 0.11974144, 0.02280299]),\n",
       " array([0.07016537, 0.2017314 , 0.24386596, 0.35122238, 0.13301489]),\n",
       " array([0.05549783, 0.31632491, 0.22602484, 0.30289931, 0.09925312]),\n",
       " array([0.00511733, 0.02220277, 0.27672364, 0.33069386, 0.3652624 ]),\n",
       " array([0.00326145, 0.02560837, 0.17530638, 0.6448014 , 0.15102239]),\n",
       " array([0.0204234 , 0.17099987, 0.48349295, 0.30927374, 0.01581005]),\n",
       " array([0.13702989, 0.16835007, 0.11011885, 0.25351384, 0.33098736]),\n",
       " array([0.022481  , 0.01538268, 0.01984846, 0.03367553, 0.90861233]),\n",
       " array([0.01514737, 0.11438404, 0.74493179, 0.06464497, 0.06089183]),\n",
       " array([0.0205663 , 0.07057495, 0.05590749, 0.255035  , 0.59791627]),\n",
       " array([0.45886113, 0.31182872, 0.04895106, 0.0890937 , 0.09126539]),\n",
       " array([0.10331412, 0.32082127, 0.25048577, 0.2569889 , 0.06838993]),\n",
       " array([0.48396005, 0.15418817, 0.05115707, 0.02240168, 0.28829303]),\n",
       " array([0.19556498, 0.19150982, 0.40456376, 0.18276772, 0.02559372]),\n",
       " array([0.05049001, 0.17188258, 0.56034608, 0.20130626, 0.01597508]),\n",
       " array([0.00738624, 0.02052642, 0.17809   , 0.43276087, 0.36123646]),\n",
       " array([0.57480631, 0.26269192, 0.07364371, 0.06662766, 0.0222304 ]),\n",
       " array([0.03283348, 0.11165508, 0.0957519 , 0.41277802, 0.34698152]),\n",
       " array([0.02530433, 0.07644349, 0.15441912, 0.2563262 , 0.48750686]),\n",
       " array([0.02969577, 0.02886539, 0.02457054, 0.10616079, 0.81070751]),\n",
       " array([0.02414004, 0.01327402, 0.10354146, 0.27136852, 0.58767596]),\n",
       " array([0.04660436, 0.04602394, 0.04391092, 0.18630616, 0.67715463]),\n",
       " array([0.32726898, 0.22704494, 0.09064838, 0.10872711, 0.24631059]),\n",
       " array([0.06757334, 0.14050237, 0.28425865, 0.29252506, 0.21514058]),\n",
       " array([0.03831913, 0.03835143, 0.03811621, 0.23060832, 0.65460491]),\n",
       " array([0.03485526, 0.06732126, 0.06180791, 0.1203248 , 0.71569077]),\n",
       " array([0.70135752, 0.2131466 , 0.03836106, 0.03060318, 0.01653164]),\n",
       " array([0.75505091, 0.12780042, 0.05298872, 0.02714682, 0.03701313]),\n",
       " array([0.05143955, 0.07329181, 0.20518978, 0.40952417, 0.26055469]),\n",
       " array([0.0449808 , 0.49131506, 0.33799863, 0.10387432, 0.02183118]),\n",
       " array([0.0198379 , 0.10705842, 0.5461307 , 0.30674355, 0.02022943]),\n",
       " array([0.0054687 , 0.06551907, 0.2734245 , 0.30371856, 0.35186917]),\n",
       " array([0.28019573, 0.48604429, 0.16719807, 0.0387734 , 0.02778851]),\n",
       " array([0.51034363, 0.38204857, 0.07789163, 0.01805101, 0.01166516]),\n",
       " array([0.02776188, 0.31686907, 0.27524426, 0.28603574, 0.09408906]),\n",
       " array([0.15337558, 0.11022917, 0.08083216, 0.14768112, 0.50788197]),\n",
       " array([0.04167476, 0.06791171, 0.06633725, 0.29886288, 0.52521341]),\n",
       " array([0.25863683, 0.50080421, 0.10343274, 0.0472284 , 0.08989782]),\n",
       " array([0.80474327, 0.12713126, 0.03377152, 0.01562188, 0.01873208]),\n",
       " array([0.03172074, 0.38100935, 0.49915014, 0.08061291, 0.00750686]),\n",
       " array([0.33708026, 0.29070168, 0.1543071 , 0.06458975, 0.1533212 ]),\n",
       " array([0.07600656, 0.32497488, 0.45156171, 0.11836626, 0.02909059]),\n",
       " array([0.41143202, 0.09412415, 0.09867426, 0.14205044, 0.25371914]),\n",
       " array([0.59907646, 0.22714038, 0.08291124, 0.03807498, 0.05279693]),\n",
       " array([0.03376276, 0.31715555, 0.34719529, 0.27954237, 0.02234403]),\n",
       " array([0.54212266, 0.08842565, 0.08548382, 0.03069697, 0.25327089]),\n",
       " array([0.01108527, 0.07344576, 0.5472315 , 0.31440608, 0.0538314 ]),\n",
       " array([0.09161586, 0.49587232, 0.27161145, 0.07717762, 0.06372274]),\n",
       " array([0.00538244, 0.11402292, 0.2608166 , 0.55484037, 0.06493766]),\n",
       " array([0.0164453 , 0.06323436, 0.0699271 , 0.26809434, 0.58229889]),\n",
       " array([0.01753158, 0.47637145, 0.19265861, 0.26633276, 0.0471056 ]),\n",
       " array([0.04109518, 0.11260071, 0.26106697, 0.41596221, 0.16927493]),\n",
       " array([0.05412797, 0.07208803, 0.0548521 , 0.083569  , 0.7353629 ]),\n",
       " array([0.33138919, 0.5143597 , 0.08555665, 0.03819615, 0.0304983 ]),\n",
       " array([0.02677667, 0.29655996, 0.24064481, 0.36440251, 0.07161606]),\n",
       " array([0.00372629, 0.01923525, 0.08236636, 0.59143474, 0.30323737]),\n",
       " array([0.00815606, 0.02870253, 0.20706535, 0.25365811, 0.50241794]),\n",
       " array([0.02715877, 0.05846575, 0.06177571, 0.17629516, 0.67630461]),\n",
       " array([0.16862205, 0.30635831, 0.35586362, 0.12397317, 0.04518286]),\n",
       " array([0.13088873, 0.42315491, 0.19687456, 0.20369849, 0.04538331]),\n",
       " array([0.09475785, 0.22908166, 0.52644139, 0.07022487, 0.07949422]),\n",
       " array([0.04605676, 0.69606338, 0.15516298, 0.04773477, 0.05498211]),\n",
       " array([0.0018894 , 0.03018998, 0.45146959, 0.44016998, 0.07628104]),\n",
       " array([0.18523261, 0.34171848, 0.2225315 , 0.13581595, 0.11470147]),\n",
       " array([0.09923168, 0.66004166, 0.15233166, 0.0629999 , 0.02539509]),\n",
       " array([0.00412486, 0.03132642, 0.03249515, 0.35113782, 0.58091575]),\n",
       " array([0.00867329, 0.01160485, 0.11019872, 0.47704307, 0.39248008]),\n",
       " array([0.11974147, 0.37880573, 0.16012694, 0.09360832, 0.24771755]),\n",
       " array([0.36393438, 0.27567934, 0.21714124, 0.11924235, 0.02400269]),\n",
       " array([0.0183069 , 0.07458576, 0.43871213, 0.44574409, 0.02265113]),\n",
       " array([0.0026391 , 0.03691234, 0.18121201, 0.54311891, 0.23611763]),\n",
       " array([0.05292403, 0.11221859, 0.13345798, 0.50068317, 0.20071624]),\n",
       " array([0.06002162, 0.05792733, 0.04488697, 0.33786171, 0.49930236]),\n",
       " array([0.08584304, 0.17918615, 0.58233032, 0.10382406, 0.04881643]),\n",
       " array([0.04150426, 0.18282042, 0.28483546, 0.29318379, 0.19765607]),\n",
       " array([0.09341547, 0.32789576, 0.35597217, 0.1294615 , 0.0932551 ]),\n",
       " array([0.03260924, 0.45237542, 0.39317128, 0.11020576, 0.0116383 ]),\n",
       " array([0.32339849, 0.1984237 , 0.18982171, 0.1825544 , 0.10580171]),\n",
       " array([0.44125217, 0.38031011, 0.03849875, 0.09510599, 0.04483298]),\n",
       " array([0.39553672, 0.19476128, 0.06828049, 0.25415911, 0.0872624 ]),\n",
       " array([0.1420119 , 0.09360464, 0.18984845, 0.35795741, 0.2165776 ]),\n",
       " array([0.00962724, 0.07472138, 0.26646473, 0.57670649, 0.07248016]),\n",
       " array([0.03952084, 0.27073736, 0.24884033, 0.34465167, 0.0962498 ]),\n",
       " array([0.88541904, 0.04466867, 0.01442071, 0.01029397, 0.0451976 ]),\n",
       " array([0.02272791, 0.06126018, 0.40600246, 0.43288628, 0.07712317]),\n",
       " array([0.59356149, 0.23499848, 0.08677981, 0.0292467 , 0.05541352]),\n",
       " array([0.06582324, 0.49029004, 0.3413777 , 0.05015438, 0.05235464]),\n",
       " array([0.03329533, 0.41629282, 0.49879593, 0.03643961, 0.01517631]),\n",
       " array([0.82555312, 0.09043476, 0.02190556, 0.02445796, 0.0376486 ]),\n",
       " array([0.08926664, 0.02536273, 0.04040669, 0.10415928, 0.74080466]),\n",
       " array([0.01034493, 0.02543616, 0.0532108 , 0.21146738, 0.69954073]),\n",
       " array([0.12180613, 0.15628068, 0.05031884, 0.14648193, 0.52511241]),\n",
       " array([0.03521327, 0.07781465, 0.02405304, 0.07785842, 0.78506062]),\n",
       " array([0.02276711, 0.12648031, 0.23834998, 0.38665536, 0.22574724]),\n",
       " array([0.16659792, 0.6127916 , 0.20590385, 0.01170682, 0.00299981]),\n",
       " array([0.032645  , 0.011471  , 0.03429497, 0.07933254, 0.84225649]),\n",
       " array([0.00942982, 0.07474842, 0.1401958 , 0.42452414, 0.35110182]),\n",
       " array([0.55889794, 0.1924683 , 0.07509689, 0.04919593, 0.12434094]),\n",
       " array([0.68196004, 0.19413805, 0.0195088 , 0.01324922, 0.09114389]),\n",
       " array([0.16089725, 0.20106515, 0.2226675 , 0.20136265, 0.21400746]),\n",
       " array([0.01290406, 0.07624121, 0.44467999, 0.37718205, 0.08899269]),\n",
       " array([0.13936288, 0.0448164 , 0.08261593, 0.44676991, 0.28643487]),\n",
       " array([0.04697374, 0.12621365, 0.15286837, 0.46226848, 0.21167577]),\n",
       " array([0.04804534, 0.28461342, 0.44610802, 0.16678   , 0.05445323]),\n",
       " array([0.01177278, 0.13514107, 0.47077097, 0.31834337, 0.06397181]),\n",
       " array([0.05558092, 0.10999226, 0.08441156, 0.2849259 , 0.46508937]),\n",
       " array([0.32051186, 0.21652449, 0.19509078, 0.0963633 , 0.17150956]),\n",
       " array([0.00907476, 0.07932692, 0.27396099, 0.53598369, 0.10165364]),\n",
       " array([0.04683483, 0.13560645, 0.47241524, 0.28894888, 0.0561946 ]),\n",
       " array([0.43920031, 0.27616797, 0.12071837, 0.03723486, 0.12667849]),\n",
       " array([0.21385069, 0.476383  , 0.2467592 , 0.04715866, 0.01584845]),\n",
       " array([0.00562998, 0.07647265, 0.21012657, 0.5074797 , 0.2002911 ]),\n",
       " array([0.0167165 , 0.03750413, 0.43020636, 0.39889424, 0.11667877]),\n",
       " array([0.63663586, 0.13800126, 0.06059729, 0.04323092, 0.12153467]),\n",
       " array([0.00687135, 0.03809649, 0.16191631, 0.51955526, 0.27356058]),\n",
       " array([0.63437111, 0.32415355, 0.03008609, 0.00961295, 0.0017763 ]),\n",
       " array([0.03700502, 0.03158027, 0.04390919, 0.22671017, 0.66079534]),\n",
       " array([0.59927961, 0.16931492, 0.05523222, 0.07844132, 0.09773194]),\n",
       " array([0.60586533, 0.26568502, 0.0523482 , 0.02642378, 0.04967767]),\n",
       " array([0.01235384, 0.45467143, 0.42044701, 0.083603  , 0.02892472]),\n",
       " array([0.12619239, 0.0920976 , 0.07534566, 0.31846023, 0.38790412]),\n",
       " array([0.09680368, 0.3296055 , 0.36973742, 0.15810095, 0.04575244]),\n",
       " array([0.01259476, 0.08150372, 0.25042097, 0.57688369, 0.07859686]),\n",
       " array([0.04813746, 0.0630495 , 0.04512911, 0.36538998, 0.47829395]),\n",
       " array([0.17180655, 0.34522106, 0.30275873, 0.09251862, 0.08769503]),\n",
       " array([0.0093152 , 0.11579844, 0.23790895, 0.38665907, 0.25031834]),\n",
       " array([0.21242364, 0.42618268, 0.18260684, 0.15798684, 0.0208    ]),\n",
       " array([0.00599713, 0.0194378 , 0.05661823, 0.33515138, 0.58279546]),\n",
       " array([0.00246795, 0.04348585, 0.35280069, 0.51109876, 0.09014675]),\n",
       " array([0.91697914, 0.04236335, 0.00719283, 0.00711442, 0.02635027]),\n",
       " array([0.25250566, 0.19473581, 0.12026978, 0.11189779, 0.32059095]),\n",
       " array([0.02317069, 0.04834089, 0.18353948, 0.39540505, 0.34954389]),\n",
       " array([0.02987198, 0.05628146, 0.02923666, 0.10199196, 0.78261794]),\n",
       " array([0.06101476, 0.38372197, 0.3498885 , 0.1826911 , 0.02268367]),\n",
       " array([0.52652347, 0.21468674, 0.0962519 , 0.07316937, 0.08936852]),\n",
       " array([0.68995729, 0.1866375 , 0.07256044, 0.02514773, 0.02569704]),\n",
       " array([0.02298738, 0.16019872, 0.42264204, 0.33978348, 0.05438838]),\n",
       " array([0.00323903, 0.00468433, 0.0148842 , 0.4554638 , 0.52172864]),\n",
       " array([0.01987346, 0.06152076, 0.26508412, 0.34153482, 0.31198684]),\n",
       " array([0.24497234, 0.42638396, 0.12445528, 0.12487659, 0.07931183]),\n",
       " array([0.0323746 , 0.23154259, 0.23686003, 0.37820552, 0.12101726]),\n",
       " array([0.34360421, 0.28813116, 0.27710131, 0.06782235, 0.02334097]),\n",
       " array([0.82076179, 0.12924353, 0.03477564, 0.00613978, 0.00907925]),\n",
       " array([0.23241163, 0.48244887, 0.20804934, 0.06034231, 0.01674785]),\n",
       " array([0.0545372 , 0.25335003, 0.38646538, 0.23038748, 0.0752599 ]),\n",
       " array([0.13554963, 0.65375959, 0.14235397, 0.0271901 , 0.0411467 ]),\n",
       " array([0.05486122, 0.08335973, 0.15854867, 0.3134267 , 0.38980368]),\n",
       " array([0.86400804, 0.09598534, 0.01248616, 0.00743642, 0.02008404]),\n",
       " array([0.01112397, 0.15847056, 0.29982334, 0.48174259, 0.04883954]),\n",
       " array([0.38803244, 0.16424294, 0.06282487, 0.18392908, 0.20097067]),\n",
       " array([0.06974754, 0.01725681, 0.03522275, 0.13914151, 0.73863138]),\n",
       " array([0.48250297, 0.37280164, 0.08258237, 0.01694789, 0.04516513]),\n",
       " array([0.01515962, 0.0118892 , 0.05263133, 0.41297648, 0.50734338]),\n",
       " array([0.11073931, 0.06942208, 0.16167597, 0.47649008, 0.18167256]),\n",
       " array([0.74937906, 0.1832498 , 0.0375236 , 0.01718411, 0.01266342]),\n",
       " array([0.05566619, 0.06824514, 0.09221989, 0.1905185 , 0.59335027]),\n",
       " array([0.79737186, 0.09630438, 0.05419114, 0.01764462, 0.03448801]),\n",
       " array([0.08679748, 0.08241378, 0.16744026, 0.49411096, 0.16923752]),\n",
       " array([0.02545807, 0.39815407, 0.3749624 , 0.15519924, 0.04622623]),\n",
       " array([0.00437871, 0.03641913, 0.47950896, 0.4355074 , 0.04418579]),\n",
       " array([0.07650752, 0.12702957, 0.34262978, 0.22160975, 0.23222338]),\n",
       " array([0.79907944, 0.10691349, 0.03140801, 0.03436223, 0.02823683]),\n",
       " array([0.11166394, 0.33203571, 0.33721027, 0.1936446 , 0.02544548]),\n",
       " array([0.1708847 , 0.62712947, 0.08871052, 0.05037333, 0.06290197]),\n",
       " array([0.02603804, 0.01956227, 0.03715193, 0.17756792, 0.73967985]),\n",
       " array([0.03776639, 0.15983713, 0.56824551, 0.18856363, 0.04558733]),\n",
       " array([0.0253506 , 0.31082967, 0.42907067, 0.19935108, 0.03539797]),\n",
       " array([0.61385855, 0.27549879, 0.05643752, 0.02701969, 0.02718545]),\n",
       " array([0.09815123, 0.22088642, 0.33113307, 0.26540291, 0.08442638]),\n",
       " array([0.65199404, 0.13991469, 0.09970828, 0.03247419, 0.0759088 ]),\n",
       " array([0.14892074, 0.12052528, 0.02816797, 0.07527531, 0.62711071]),\n",
       " array([0.06138708, 0.34951231, 0.3607053 , 0.14548764, 0.08290766]),\n",
       " array([0.57526943, 0.19582596, 0.18431552, 0.02732339, 0.01726571]),\n",
       " array([0.1507844 , 0.58787167, 0.19896318, 0.0451795 , 0.01720125]),\n",
       " array([0.40200962, 0.37018353, 0.12141565, 0.03211544, 0.07427576]),\n",
       " array([0.53361355, 0.0692218 , 0.06790383, 0.1054001 , 0.22386071]),\n",
       " array([0.02260426, 0.0782015 , 0.1795232 , 0.59424126, 0.12542977]),\n",
       " array([0.24636541, 0.36623436, 0.27451965, 0.08565826, 0.02722232]),\n",
       " array([0.02834767, 0.28577625, 0.30893967, 0.35932898, 0.01760743]),\n",
       " array([0.65545215, 0.13972053, 0.0680131 , 0.03845768, 0.09835654]),\n",
       " array([0.00328816, 0.01570385, 0.01812328, 0.22281321, 0.74007151]),\n",
       " array([0.07908707, 0.00903633, 0.00667802, 0.2542091 , 0.65098948]),\n",
       " array([0.5646347 , 0.36070147, 0.04617698, 0.01512453, 0.01336233]),\n",
       " array([0.16615186, 0.45640359, 0.25682417, 0.11091227, 0.00970812]),\n",
       " array([0.49707947, 0.30926885, 0.06553489, 0.07485649, 0.0532603 ]),\n",
       " array([0.44533214, 0.46501084, 0.06416047, 0.01274891, 0.01274764]),\n",
       " array([0.37487193, 0.47916607, 0.08348698, 0.048279  , 0.01419602]),\n",
       " array([0.01302264, 0.01796287, 0.03462181, 0.15894839, 0.77544429]),\n",
       " array([0.26146359, 0.17916397, 0.25950556, 0.21597491, 0.08389197]),\n",
       " array([0.01518749, 0.22795278, 0.43133481, 0.23203071, 0.09349421]),\n",
       " array([0.00338887, 0.03135346, 0.17445266, 0.64217883, 0.14862617]),\n",
       " array([0.70789222, 0.11125903, 0.10367797, 0.02796973, 0.04920105]),\n",
       " array([0.32133235, 0.32290399, 0.12629746, 0.06960574, 0.15986046]),\n",
       " array([0.03230211, 0.13861618, 0.21054579, 0.52390949, 0.09462644]),\n",
       " array([0.80981782, 0.02115698, 0.02200447, 0.06529213, 0.0817286 ]),\n",
       " array([0.15573065, 0.18211458, 0.44872685, 0.09157927, 0.12184865]),\n",
       " array([0.35956363, 0.45713308, 0.14456014, 0.03361819, 0.00512496]),\n",
       " array([0.05396477, 0.10545361, 0.20169364, 0.32394114, 0.31494685]),\n",
       " array([0.05039072, 0.24185488, 0.40597295, 0.28664367, 0.01513778]),\n",
       " array([0.11124301, 0.47141572, 0.27476803, 0.12911878, 0.01345446]),\n",
       " array([0.03307217, 0.06867724, 0.3291969 , 0.31329376, 0.25575994]),\n",
       " array([0.11809366, 0.23826448, 0.53564234, 0.0740228 , 0.03397673]),\n",
       " array([0.00921745, 0.08710857, 0.44674396, 0.30222641, 0.15470361]),\n",
       " array([0.24539369, 0.03754676, 0.02061557, 0.02702301, 0.66942098]),\n",
       " array([0.15823405, 0.40682754, 0.26366394, 0.10598504, 0.06528944]),\n",
       " array([0.01917609, 0.03793876, 0.07633623, 0.36835415, 0.49819477]),\n",
       " array([0.25814298, 0.37397249, 0.28096662, 0.06632213, 0.02059578]),\n",
       " array([0.0441018 , 0.1924398 , 0.24747672, 0.35591915, 0.16006253]),\n",
       " array([0.08644229, 0.15571783, 0.16558984, 0.23318773, 0.35906232]),\n",
       " array([0.04151614, 0.06065828, 0.06473238, 0.42409782, 0.40899539]),\n",
       " array([0.38284302, 0.31204624, 0.1353373 , 0.10315448, 0.06661897]),\n",
       " array([0.00360222, 0.05588434, 0.35651643, 0.53292318, 0.05107383]),\n",
       " array([0.13385944, 0.08981663, 0.16396076, 0.30722546, 0.3051377 ]),\n",
       " array([0.02682751, 0.291623  , 0.52084887, 0.12712302, 0.03357759]),\n",
       " array([0.17024371, 0.43123816, 0.25313436, 0.07805547, 0.0673283 ]),\n",
       " array([0.87697292, 0.08233735, 0.01758687, 0.01238135, 0.01072152]),\n",
       " array([0.03778873, 0.14172465, 0.37550677, 0.33905504, 0.10592481]),\n",
       " array([0.68307955, 0.12457429, 0.01359342, 0.02387768, 0.15487506]),\n",
       " array([0.76973206, 0.19569222, 0.01029004, 0.00620782, 0.01807786]),\n",
       " array([0.09423744, 0.55251138, 0.19146729, 0.12124133, 0.04054256]),\n",
       " array([0.01788903, 0.18509387, 0.36812901, 0.39834475, 0.03054334]),\n",
       " array([0.16733241, 0.28878943, 0.45294702, 0.07810589, 0.01282526]),\n",
       " array([0.00697917, 0.37129689, 0.52525634, 0.09135155, 0.00511606]),\n",
       " array([0.26477701, 0.06726281, 0.1047571 , 0.18331537, 0.37988771]),\n",
       " array([0.00530067, 0.02112526, 0.02868696, 0.32852861, 0.61635849]),\n",
       " array([0.06775291, 0.14457668, 0.44078339, 0.28854199, 0.05834503]),\n",
       " array([0.02120279, 0.04368984, 0.17154562, 0.28581348, 0.47774826]),\n",
       " array([0.04497227, 0.16666653, 0.40004803, 0.28754568, 0.10076749]),\n",
       " array([0.00224353, 0.02966249, 0.1927333 , 0.63553752, 0.13982317]),\n",
       " array([0.0024264 , 0.00803935, 0.03389444, 0.49891715, 0.45672265]),\n",
       " array([0.00548946, 0.08171953, 0.29768995, 0.32800258, 0.28709848]),\n",
       " array([0.00429025, 0.09556445, 0.42621681, 0.44210193, 0.03182655]),\n",
       " array([0.1841603 , 0.38988617, 0.25345636, 0.10147087, 0.0710263 ]),\n",
       " array([0.08347768, 0.41140903, 0.22753305, 0.23301679, 0.04456345]),\n",
       " array([0.00329063, 0.04646455, 0.44707381, 0.46745892, 0.03571209]),\n",
       " array([0.03123687, 0.07947763, 0.16244887, 0.28678034, 0.44005629]),\n",
       " array([0.03428793, 0.01526948, 0.05637405, 0.08773595, 0.8063326 ]),\n",
       " array([0.34540446, 0.26620371, 0.20723198, 0.14952034, 0.03163951]),\n",
       " array([0.02525635, 0.04494371, 0.18646923, 0.5943309 , 0.1489998 ]),\n",
       " array([0.00683767, 0.04344705, 0.25683887, 0.29738534, 0.39549107]),\n",
       " array([0.02542286, 0.04733962, 0.08224966, 0.18790762, 0.65708025]),\n",
       " array([0.01760451, 0.08368955, 0.08481608, 0.50881272, 0.30507714]),\n",
       " array([0.09883415, 0.40492782, 0.24892232, 0.14578349, 0.10153222]),\n",
       " array([0.55238967, 0.15831698, 0.06031077, 0.1110329 , 0.11794969]),\n",
       " array([0.04899318, 0.31824761, 0.42071443, 0.19588344, 0.01616135]),\n",
       " array([0.15953869, 0.24172611, 0.20137935, 0.29258957, 0.10476628]),\n",
       " array([0.75104548, 0.1538801 , 0.05123863, 0.02315535, 0.02068044]),\n",
       " array([0.31981983, 0.15478601, 0.2907582 , 0.15180275, 0.08283321]),\n",
       " array([0.00633994, 0.1290819 , 0.47695571, 0.25199638, 0.13562607]),\n",
       " array([0.0184545 , 0.09438469, 0.34535101, 0.45227869, 0.08953111]),\n",
       " array([0.08935176, 0.03352741, 0.01269903, 0.10834171, 0.7560801 ]),\n",
       " array([0.01147556, 0.13606865, 0.44484682, 0.35582036, 0.05178862]),\n",
       " array([0.01988874, 0.05572325, 0.06604617, 0.69750542, 0.16083642]),\n",
       " array([0.03396322, 0.02306634, 0.18738114, 0.26036115, 0.49522816]),\n",
       " array([0.38765652, 0.42315417, 0.13070917, 0.04570434, 0.01277581]),\n",
       " array([0.07034201, 0.38916703, 0.29242187, 0.22930649, 0.01876261]),\n",
       " array([0.01676937, 0.01315656, 0.0397321 , 0.33189309, 0.59844889]),\n",
       " array([0.01440891, 0.01223312, 0.02093903, 0.16559497, 0.78682396]),\n",
       " array([0.04197515, 0.10174639, 0.25986255, 0.49298644, 0.10342947]),\n",
       " array([0.02477772, 0.06284132, 0.05953434, 0.12016033, 0.73268629]),\n",
       " array([0.05182926, 0.36281038, 0.28729191, 0.27565641, 0.02241204]),\n",
       " array([0.03815745, 0.03236994, 0.14783965, 0.33354207, 0.44809089]),\n",
       " array([0.0024102 , 0.00288948, 0.01411381, 0.1795266 , 0.80105991]),\n",
       " array([0.04139559, 0.4273369 , 0.35672801, 0.16926537, 0.00527412]),\n",
       " array([0.02618541, 0.30505603, 0.46966791, 0.16054029, 0.03855035]),\n",
       " array([0.01304015, 0.0398994 , 0.02720057, 0.2787514 , 0.64110848]),\n",
       " array([0.13540332, 0.26403803, 0.28837032, 0.09915806, 0.21303027]),\n",
       " array([0.57172279, 0.26564138, 0.08868418, 0.06812974, 0.00582191]),\n",
       " array([0.13294449, 0.23847218, 0.29934879, 0.23394226, 0.09529228]),\n",
       " array([0.00420075, 0.0057235 , 0.02966966, 0.24364342, 0.71676268]),\n",
       " array([0.02414971, 0.20247066, 0.24371296, 0.27764967, 0.252017  ]),\n",
       " array([0.06452971, 0.49051656, 0.34330495, 0.09407437, 0.00757441]),\n",
       " array([0.58582451, 0.27832324, 0.06940079, 0.0099932 , 0.05645826]),\n",
       " array([0.42713577, 0.3329013 , 0.10100126, 0.07201048, 0.06695119]),\n",
       " array([0.25570982, 0.44879553, 0.2485448 , 0.04191997, 0.00502988]),\n",
       " array([0.30785624, 0.06839917, 0.20440578, 0.15923033, 0.26010847]),\n",
       " array([0.0084749 , 0.05541397, 0.3948356 , 0.46464173, 0.0766338 ]),\n",
       " array([0.31101894, 0.19818641, 0.14586349, 0.25231592, 0.09261525]),\n",
       " array([0.01521387, 0.00836915, 0.0400797 , 0.22274262, 0.71359466]),\n",
       " array([0.38033321, 0.31048624, 0.12692854, 0.128323  , 0.053929  ]),\n",
       " array([0.23870166, 0.55691032, 0.13915489, 0.04007821, 0.02515492]),\n",
       " array([0.55798935, 0.28401925, 0.03407049, 0.02270553, 0.10121537]),\n",
       " array([0.21849864, 0.39622004, 0.29920627, 0.07456842, 0.01150663]),\n",
       " array([0.03502344, 0.16143073, 0.33828305, 0.41406834, 0.05119444]),\n",
       " array([0.68796065, 0.09362745, 0.13297531, 0.05693684, 0.02849974]),\n",
       " array([0.04372021, 0.1321905 , 0.27550397, 0.11384184, 0.43474349]),\n",
       " array([0.01833107, 0.01040893, 0.02440454, 0.29859897, 0.64825649]),\n",
       " array([0.08989799, 0.14375751, 0.10607165, 0.14114556, 0.51912729]),\n",
       " array([0.2802033 , 0.46084634, 0.09901673, 0.07448034, 0.08545329]),\n",
       " array([0.097286  , 0.49874103, 0.34866854, 0.04477616, 0.01052827]),\n",
       " array([0.6321854 , 0.17100588, 0.05356961, 0.12506115, 0.01817796]),\n",
       " array([0.51809528, 0.34474493, 0.08252157, 0.0307206 , 0.02391762]),\n",
       " array([0.07851681, 0.4622995 , 0.40261496, 0.04277292, 0.0137958 ]),\n",
       " array([0.11450295, 0.26521565, 0.42507062, 0.13896767, 0.05624311]),\n",
       " array([0.0168864 , 0.03121654, 0.08993546, 0.42918999, 0.43277161]),\n",
       " array([0.36309804, 0.10377268, 0.16702391, 0.22551272, 0.14059265]),\n",
       " array([0.00562236, 0.00473824, 0.06210193, 0.40572463, 0.52181284]),\n",
       " array([0.01027479, 0.14905489, 0.59864626, 0.08601611, 0.15600795]),\n",
       " array([0.76811432, 0.1214016 , 0.02673766, 0.01532941, 0.06841701]),\n",
       " array([0.45316913, 0.31668972, 0.11285189, 0.04998791, 0.06730135]),\n",
       " array([0.266191  , 0.39898642, 0.03736604, 0.10455609, 0.19290045]),\n",
       " array([0.00471296, 0.03894737, 0.3506164 , 0.30431994, 0.30140333]),\n",
       " array([0.05564307, 0.18582875, 0.49945616, 0.19202171, 0.06705032]),\n",
       " array([0.02178353, 0.08331642, 0.50855079, 0.33552564, 0.05082362]),\n",
       " array([0.3817891 , 0.50967491, 0.08009679, 0.00757438, 0.02086482]),\n",
       " array([0.0596585 , 0.05371412, 0.21902297, 0.52406494, 0.14353948]),\n",
       " array([0.01595563, 0.47228795, 0.42715991, 0.06614999, 0.01844653]),\n",
       " array([0.03765562, 0.19311617, 0.33208497, 0.38505538, 0.05208786]),\n",
       " array([0.02466428, 0.1620634 , 0.4126843 , 0.37670794, 0.02388008]),\n",
       " array([0.02652831, 0.14934998, 0.34621309, 0.31042123, 0.16748739]),\n",
       " array([0.297531  , 0.37622508, 0.20800278, 0.07226884, 0.0459723 ]),\n",
       " array([0.57020775, 0.21472896, 0.10680715, 0.05741688, 0.05083926]),\n",
       " array([0.5697775 , 0.34793046, 0.05075612, 0.02536356, 0.00617236]),\n",
       " array([0.47825835, 0.23813083, 0.08068568, 0.14522079, 0.05770435]),\n",
       " array([0.12385118, 0.0752488 , 0.11224298, 0.22292308, 0.46573397]),\n",
       " array([0.00364184, 0.0228116 , 0.05778922, 0.65290227, 0.26285507]),\n",
       " array([0.17585765, 0.32092505, 0.32991306, 0.1302952 , 0.04300904]),\n",
       " array([0.65359266, 0.20908071, 0.08089944, 0.02329956, 0.03312764]),\n",
       " array([0.09653053, 0.09222671, 0.1451968 , 0.16701663, 0.49902933]),\n",
       " array([0.01488723, 0.07641069, 0.15730695, 0.37311169, 0.37828343]),\n",
       " array([0.04087657, 0.02521775, 0.08119699, 0.18711612, 0.66559257]),\n",
       " array([0.32333664, 0.37604729, 0.2240867 , 0.0621309 , 0.01439848]),\n",
       " array([0.33012582, 0.44042994, 0.13973955, 0.05204347, 0.03766121]),\n",
       " array([0.04032726, 0.06135586, 0.14539049, 0.20917148, 0.54375491]),\n",
       " array([0.02509375, 0.33233486, 0.50703828, 0.11159718, 0.02393592]),\n",
       " array([0.0490908 , 0.22804646, 0.15390206, 0.44367414, 0.12528655]),\n",
       " array([0.67108673, 0.18714762, 0.08336298, 0.03150763, 0.02689503]),\n",
       " array([0.91582185, 0.04231873, 0.00582856, 0.01181468, 0.02421618]),\n",
       " array([0.04432215, 0.13539169, 0.40022985, 0.38078162, 0.03927469]),\n",
       " array([0.00947663, 0.02165603, 0.09013919, 0.56485152, 0.31387663]),\n",
       " array([0.62445926, 0.26971423, 0.0330964 , 0.02338658, 0.04934353]),\n",
       " array([0.00401073, 0.00881624, 0.01335751, 0.0946312 , 0.87918432]),\n",
       " array([0.00412346, 0.01898153, 0.16884021, 0.44688963, 0.36116517]),\n",
       " array([0.00817698, 0.04676741, 0.24943403, 0.47519504, 0.22042653]),\n",
       " array([0.00878088, 0.07222941, 0.30097219, 0.51350886, 0.10450866]),\n",
       " array([0.01961407, 0.2588287 , 0.24608804, 0.41216898, 0.0633002 ]),\n",
       " array([0.0264856 , 0.23285409, 0.11279102, 0.2448736 , 0.38299569]),\n",
       " array([0.14823738, 0.10300478, 0.04045659, 0.13913494, 0.56916631]),\n",
       " array([0.00634237, 0.12232005, 0.55250031, 0.29756   , 0.02127728]),\n",
       " array([0.10867559, 0.11337641, 0.28112507, 0.24259727, 0.25422566]),\n",
       " array([0.70802977, 0.10817855, 0.04853147, 0.03391586, 0.10134435]),\n",
       " array([0.03173409, 0.30712154, 0.52722252, 0.08544013, 0.04848172]),\n",
       " array([0.04153744, 0.03617849, 0.07547336, 0.4605579 , 0.38625281]),\n",
       " array([0.68894859, 0.21533164, 0.02010586, 0.00871037, 0.06690353]),\n",
       " array([0.28934496, 0.22834665, 0.26392312, 0.16555525, 0.05283002]),\n",
       " array([0.05079566, 0.3110112 , 0.15710062, 0.15925473, 0.32183779]),\n",
       " array([0.0103572 , 0.05051588, 0.23440486, 0.47016663, 0.23455543]),\n",
       " array([0.22050618, 0.2187131 , 0.17236571, 0.24416802, 0.14424698]),\n",
       " array([0.04466388, 0.21105896, 0.23298204, 0.35900394, 0.15229119]),\n",
       " array([0.03118902, 0.02983057, 0.17803697, 0.50730286, 0.25364058]),\n",
       " array([0.10108468, 0.07952789, 0.08663521, 0.17708932, 0.55566291]),\n",
       " array([0.41709826, 0.44913845, 0.11091141, 0.0158244 , 0.00702748]),\n",
       " array([0.25485226, 0.05981154, 0.05660887, 0.36065505, 0.26807229]),\n",
       " array([0.38811968, 0.48058021, 0.04741938, 0.03872494, 0.04515579]),\n",
       " array([0.35739568, 0.28277038, 0.11839714, 0.04733959, 0.19409721]),\n",
       " array([0.6280677 , 0.09437642, 0.17734719, 0.03401731, 0.06619137]),\n",
       " array([0.73257224, 0.21867381, 0.01625223, 0.01600035, 0.01650137]),\n",
       " array([0.07521869, 0.52201015, 0.24719704, 0.08960061, 0.06597351]),\n",
       " array([0.1768808 , 0.45309027, 0.21093763, 0.12795053, 0.03114077]),\n",
       " array([0.02545066, 0.25344843, 0.49471081, 0.17711389, 0.04927622]),\n",
       " array([0.01584235, 0.15439249, 0.60757154, 0.21379292, 0.00840071]),\n",
       " array([0.05165601, 0.21359836, 0.30659407, 0.29279003, 0.13536153]),\n",
       " array([0.07666864, 0.56349546, 0.19737882, 0.11079878, 0.0516583 ]),\n",
       " array([0.77409698, 0.17020345, 0.01688331, 0.02010887, 0.0187074 ]),\n",
       " array([0.01700146, 0.2213751 , 0.28366395, 0.2617083 , 0.21625118]),\n",
       " array([0.03751552, 0.14663491, 0.15437278, 0.321943  , 0.3395338 ]),\n",
       " array([0.31533457, 0.35759692, 0.27126482, 0.04502892, 0.01077477]),\n",
       " array([0.36897998, 0.38515784, 0.0788647 , 0.03659263, 0.13040485]),\n",
       " array([0.21710958, 0.05194681, 0.08579264, 0.09531793, 0.54983304]),\n",
       " array([0.01768549, 0.15319594, 0.4565917 , 0.34682891, 0.02569795]),\n",
       " array([0.0309391 , 0.35964345, 0.45576195, 0.1346279 , 0.0190276 ]),\n",
       " array([0.20636629, 0.32407226, 0.22961771, 0.12278721, 0.11715653]),\n",
       " array([0.03161206, 0.21018251, 0.32371667, 0.29161729, 0.14287147]),\n",
       " array([0.45187965, 0.25364788, 0.11475204, 0.07950428, 0.10021615]),\n",
       " array([0.49247284, 0.23149192, 0.12123884, 0.08501301, 0.06978339]),\n",
       " array([0.03890553, 0.28721434, 0.10901676, 0.18835854, 0.37650484]),\n",
       " array([0.1346046 , 0.05854764, 0.20162801, 0.38350519, 0.22171456]),\n",
       " array([0.01798241, 0.27043407, 0.20795647, 0.27566464, 0.22796242]),\n",
       " array([0.81986164, 0.11253087, 0.02429427, 0.01399165, 0.02932158]),\n",
       " array([0.63073066, 0.21480109, 0.09091381, 0.03222199, 0.03133246]),\n",
       " array([0.57786638, 0.20501654, 0.10233778, 0.03465432, 0.08012497]),\n",
       " array([0.38183068, 0.34521372, 0.09415667, 0.11335154, 0.06544738]),\n",
       " array([0.06700493, 0.29498141, 0.26270467, 0.23879503, 0.13651397]),\n",
       " array([0.01484297, 0.01760817, 0.05086073, 0.4798142 , 0.43687393]),\n",
       " array([0.02989613, 0.31950288, 0.57551456, 0.04179855, 0.03328789]),\n",
       " array([0.05207117, 0.45296571, 0.41084088, 0.07198403, 0.01213822]),\n",
       " array([0.08868616, 0.20095667, 0.3142403 , 0.36406648, 0.0320504 ]),\n",
       " array([0.12435653, 0.63751137, 0.1189329 , 0.08210695, 0.03709225]),\n",
       " array([0.01276265, 0.02351734, 0.03880842, 0.26329752, 0.66161406]),\n",
       " array([0.78001778, 0.09737866, 0.06739703, 0.02117913, 0.0340274 ]),\n",
       " array([0.07569616, 0.38541675, 0.24782691, 0.2215427 , 0.06951747]),\n",
       " array([0.1585201 , 0.11406982, 0.05882076, 0.10393529, 0.56465403]),\n",
       " array([0.11123794, 0.53324314, 0.12900881, 0.09233453, 0.13417559]),\n",
       " array([0.53211447, 0.31495522, 0.13256989, 0.00829632, 0.01206409]),\n",
       " array([0.00364177, 0.00798925, 0.01277128, 0.22370275, 0.75189495]),\n",
       " array([0.07464029, 0.18715029, 0.47943183, 0.20142498, 0.05735261]),\n",
       " array([0.68211167, 0.16632144, 0.05993079, 0.05145617, 0.04017993]),\n",
       " array([0.55285518, 0.30630462, 0.05763894, 0.05412668, 0.02907457]),\n",
       " array([0.50416541, 0.19428724, 0.10917542, 0.14699592, 0.04537601]),\n",
       " array([0.00985442, 0.06530271, 0.26996325, 0.42569615, 0.22918347]),\n",
       " array([0.01625004, 0.35541996, 0.46428124, 0.15379529, 0.01025346]),\n",
       " array([0.00425328, 0.03399243, 0.21029073, 0.42248283, 0.32898073]),\n",
       " array([0.25204185, 0.38002931, 0.2702164 , 0.0639174 , 0.03379504]),\n",
       " array([0.78614447, 0.12444927, 0.0156703 , 0.04498663, 0.02874933]),\n",
       " array([0.64409831, 0.27587512, 0.05494627, 0.01454869, 0.01053161]),\n",
       " array([0.06430495, 0.24868515, 0.42393749, 0.22471388, 0.03835854]),\n",
       " array([0.45962177, 0.33482543, 0.13823313, 0.04662146, 0.0206982 ]),\n",
       " array([0.54576773, 0.37289697, 0.04234444, 0.03171156, 0.0072793 ]),\n",
       " array([0.08389706, 0.14216035, 0.08973023, 0.14727352, 0.53693883]),\n",
       " array([0.51418602, 0.39450306, 0.04623523, 0.02385239, 0.0212233 ]),\n",
       " array([0.06778044, 0.25678435, 0.26594431, 0.17794216, 0.23154874]),\n",
       " array([0.02038217, 0.05739787, 0.09694038, 0.66706539, 0.15821419]),\n",
       " array([0.0226324 , 0.20414242, 0.49633367, 0.26076826, 0.01612325]),\n",
       " array([0.01787336, 0.09969842, 0.35314201, 0.47672777, 0.05255844]),\n",
       " array([0.00464507, 0.0186478 , 0.19812553, 0.56873158, 0.20985002]),\n",
       " array([0.02265003, 0.02523524, 0.05806553, 0.26258236, 0.63146685]),\n",
       " array([0.81506682, 0.09218001, 0.01534964, 0.0178389 , 0.05956462]),\n",
       " array([0.50941931, 0.31799907, 0.07679045, 0.03389425, 0.06189692]),\n",
       " array([0.25975329, 0.39606789, 0.20720825, 0.07677656, 0.06019401]),\n",
       " array([0.03264917, 0.30347489, 0.57868472, 0.06561118, 0.01958004]),\n",
       " array([6.29243060e-01, 7.28637900e-03, 5.96310644e-04, 6.15368083e-03,\n",
       "        3.56720570e-01]),\n",
       " array([0.00464817, 0.23835602, 0.5124344 , 0.23711548, 0.00744593]),\n",
       " array([0.00238606, 0.06910587, 0.11612173, 0.44978728, 0.36259906]),\n",
       " array([0.65330768, 0.06706629, 0.05089266, 0.08804053, 0.14069284]),\n",
       " array([0.09660201, 0.27121063, 0.60634869, 0.01403346, 0.0118052 ]),\n",
       " array([0.07121835, 0.07196989, 0.1562134 , 0.26243739, 0.43816097]),\n",
       " array([0.0627345 , 0.05110599, 0.08247699, 0.14217831, 0.66150421]),\n",
       " array([0.20345938, 0.31785697, 0.4063431 , 0.06127523, 0.01106532]),\n",
       " array([0.48413539, 0.4289921 , 0.0522161 , 0.01370421, 0.02095221]),\n",
       " array([0.03182879, 0.03915567, 0.02841088, 0.09622543, 0.80437922]),\n",
       " array([0.02235719, 0.22219788, 0.47714542, 0.2198162 , 0.05848331]),\n",
       " array([0.01456128, 0.09595442, 0.24558907, 0.42484267, 0.21905256]),\n",
       " array([0.11837382, 0.51778104, 0.33192435, 0.01996173, 0.01195906]),\n",
       " array([0.01293085, 0.07270578, 0.47444905, 0.40639125, 0.03352307]),\n",
       " array([0.11773374, 0.50487807, 0.1926644 , 0.09840085, 0.08632293]),\n",
       " array([0.08360484, 0.1034877 , 0.44664558, 0.28930851, 0.07695338]),\n",
       " array([0.31282349, 0.18865218, 0.32832429, 0.10609071, 0.06410933]),\n",
       " array([0.05888625, 0.25513895, 0.34851621, 0.16220191, 0.17525667]),\n",
       " array([0.02316794, 0.30278738, 0.62830223, 0.04401783, 0.00172462]),\n",
       " array([0.00364859, 0.01903727, 0.11106613, 0.45925893, 0.40698908]),\n",
       " array([0.01911553, 0.15647384, 0.42833282, 0.38717156, 0.00890625]),\n",
       " array([0.0253645 , 0.1006849 , 0.08763945, 0.2795258 , 0.50678534]),\n",
       " array([0.00673646, 0.01362465, 0.02712072, 0.29927468, 0.65324349]),\n",
       " array([0.19090722, 0.44109049, 0.17624908, 0.09919732, 0.09255589]),\n",
       " array([0.56375251, 0.30828178, 0.06849746, 0.03370933, 0.02575892]),\n",
       " array([0.01430154, 0.06349512, 0.31314982, 0.2260404 , 0.38301312]),\n",
       " array([0.20731576, 0.31103132, 0.21102749, 0.09495108, 0.17567435]),\n",
       " array([0.74605316, 0.2099015 , 0.0189697 , 0.00600477, 0.01907087]),\n",
       " array([0.05000171, 0.07441158, 0.04991844, 0.1758382 , 0.64983007]),\n",
       " array([0.01089407, 0.19193828, 0.67668257, 0.09897661, 0.02150846]),\n",
       " array([0.02087563, 0.02262389, 0.03723447, 0.54823631, 0.3710297 ]),\n",
       " array([0.08826357, 0.118188  , 0.12840737, 0.20354523, 0.46159583]),\n",
       " array([0.01165406, 0.02979483, 0.16605073, 0.6169534 , 0.17554698]),\n",
       " array([0.01062818, 0.03709995, 0.24869317, 0.27067847, 0.43290024]),\n",
       " array([0.38399385, 0.40258143, 0.10711058, 0.09617377, 0.01014038]),\n",
       " array([0.04192158, 0.14921796, 0.30359141, 0.27075825, 0.2345108 ]),\n",
       " array([0.20341436, 0.18599583, 0.06737886, 0.2218578 , 0.32135316]),\n",
       " array([0.62203983, 0.13043647, 0.04007927, 0.03324925, 0.17419517]),\n",
       " array([0.55967892, 0.13895981, 0.07349986, 0.09776892, 0.13009249]),\n",
       " array([0.5598521 , 0.20247006, 0.092559  , 0.07843361, 0.06668523]),\n",
       " array([0.01363364, 0.00993604, 0.0703487 , 0.41677035, 0.48931127]),\n",
       " array([0.01179431, 0.04507737, 0.07476693, 0.31386152, 0.55449988]),\n",
       " array([0.03538382, 0.45461768, 0.33178278, 0.16094871, 0.017267  ]),\n",
       " array([0.03248374, 0.12854793, 0.179068  , 0.14628541, 0.51361493]),\n",
       " array([0.00765329, 0.02004033, 0.15547294, 0.58891179, 0.22792165]),\n",
       " array([0.00688317, 0.22233548, 0.52360845, 0.23272782, 0.01444508]),\n",
       " array([0.18570319, 0.22238619, 0.47708623, 0.08292473, 0.03189966]),\n",
       " array([0.06462969, 0.30643571, 0.40295482, 0.07783758, 0.1481422 ]),\n",
       " array([0.02669286, 0.24779963, 0.66884977, 0.03957066, 0.01708708]),\n",
       " array([0.38606781, 0.37403871, 0.16729822, 0.03959719, 0.03299807]),\n",
       " array([0.57856375, 0.24119457, 0.05226225, 0.02840644, 0.09957299]),\n",
       " array([0.07702638, 0.23072175, 0.22754907, 0.18774582, 0.27695699]),\n",
       " array([0.42845079, 0.27589761, 0.18974911, 0.04067707, 0.06522542]),\n",
       " array([0.11544147, 0.51609232, 0.25258483, 0.09076664, 0.02511474]),\n",
       " array([0.44964157, 0.26551098, 0.20152214, 0.05363675, 0.02968856]),\n",
       " array([0.05459927, 0.07764325, 0.05043997, 0.08139807, 0.73591944]),\n",
       " array([0.03529066, 0.15782557, 0.15376406, 0.54156543, 0.11155428]),\n",
       " array([0.02219951, 0.0800622 , 0.12675043, 0.5350234 , 0.23596446]),\n",
       " array([0.02559703, 0.08185048, 0.14672802, 0.43651469, 0.30930977]),\n",
       " array([0.03059316, 0.10343216, 0.18806902, 0.42067972, 0.25722594]),\n",
       " array([0.064112  , 0.03008007, 0.11735044, 0.33628632, 0.45217116]),\n",
       " array([0.02493458, 0.03276936, 0.03968661, 0.2443419 , 0.65826755]),\n",
       " array([0.85983899, 0.07477311, 0.00840759, 0.01679915, 0.04018117]),\n",
       " array([0.16103523, 0.5025829 , 0.21026576, 0.08686573, 0.03925038]),\n",
       " array([0.64490281, 0.21534516, 0.03948917, 0.0354102 , 0.06485265]),\n",
       " array([0.00548464, 0.04220074, 0.0490831 , 0.35703817, 0.54619334]),\n",
       " array([0.75089052, 0.04818152, 0.01625132, 0.06817468, 0.11650197]),\n",
       " array([0.122588  , 0.1279045 , 0.30112727, 0.28306789, 0.16531235]),\n",
       " array([0.01085808, 0.02489641, 0.52805154, 0.33448558, 0.10170839]),\n",
       " array([0.12127617, 0.43107071, 0.20452824, 0.19559162, 0.04753325]),\n",
       " array([0.03230768, 0.24339248, 0.53361762, 0.14325122, 0.04743101]),\n",
       " array([0.20926673, 0.19283337, 0.1638087 , 0.09785535, 0.33623586]),\n",
       " array([0.05835362, 0.06784189, 0.10152909, 0.47388737, 0.29838803]),\n",
       " array([0.02884365, 0.1252921 , 0.30930401, 0.43327541, 0.10328482]),\n",
       " array([0.12240162, 0.41686717, 0.31727046, 0.13393751, 0.00952325]),\n",
       " array([0.02723332, 0.2911811 , 0.34986072, 0.25018234, 0.08154252]),\n",
       " array([0.14014443, 0.30941487, 0.36136758, 0.17518544, 0.01388769]),\n",
       " array([0.01663239, 0.02616084, 0.02348675, 0.05933958, 0.87438044]),\n",
       " array([0.82981387, 0.1247877 , 0.01786995, 0.01032373, 0.01720475]),\n",
       " array([0.03514702, 0.10268154, 0.06024351, 0.17975952, 0.62216841]),\n",
       " array([0.02258681, 0.01194341, 0.01738368, 0.19123259, 0.7568535 ]),\n",
       " array([0.65353783, 0.32469841, 0.01197291, 0.00638505, 0.0034058 ]),\n",
       " array([0.59932331, 0.22477033, 0.12418851, 0.01519692, 0.03652093]),\n",
       " array([0.05159354, 0.02449049, 0.01656172, 0.04038985, 0.8669644 ]),\n",
       " array([0.31080072, 0.50024501, 0.09263064, 0.06347693, 0.0328467 ]),\n",
       " array([0.3368397 , 0.23776738, 0.18222272, 0.12595985, 0.11721035]),\n",
       " array([0.06876712, 0.34522097, 0.40426849, 0.08814504, 0.09359837]),\n",
       " array([0.06175684, 0.38247434, 0.17942685, 0.28673001, 0.08961196]),\n",
       " array([0.00863627, 0.13504755, 0.18996529, 0.58416415, 0.08218673]),\n",
       " array([0.55275982, 0.16953778, 0.13453198, 0.07451634, 0.06865409]),\n",
       " array([0.41581271, 0.2877375 , 0.14396142, 0.11347905, 0.03900932]),\n",
       " array([0.02935434, 0.24709212, 0.43496185, 0.19067787, 0.09791383]),\n",
       " array([0.01753078, 0.11232484, 0.30374214, 0.41910188, 0.14730037]),\n",
       " array([0.45432329, 0.02218472, 0.00818805, 0.01217465, 0.50312928]),\n",
       " array([0.14155207, 0.22376274, 0.38297544, 0.16920949, 0.08250027]),\n",
       " array([0.38052295, 0.37026668, 0.11856839, 0.04755463, 0.08308734]),\n",
       " array([0.05040119, 0.4279978 , 0.40272959, 0.08820727, 0.03066415]),\n",
       " array([0.06024198, 0.41307964, 0.16233343, 0.13532634, 0.2290186 ]),\n",
       " array([0.04036252, 0.03670992, 0.15591962, 0.1878793 , 0.57912864]),\n",
       " array([0.449304  , 0.40022139, 0.0756672 , 0.03142901, 0.0433784 ]),\n",
       " array([0.00245908, 0.03350145, 0.26073195, 0.37299378, 0.33031375]),\n",
       " array([0.38498423, 0.45283883, 0.12930119, 0.02173423, 0.01114151]),\n",
       " array([0.03199603, 0.15568263, 0.13891923, 0.24593534, 0.42746677]),\n",
       " array([0.44150614, 0.42776341, 0.10558235, 0.01668055, 0.00846754]),\n",
       " array([0.37895564, 0.45068611, 0.0925384 , 0.04920674, 0.02861311]),\n",
       " array([0.07292158, 0.38028707, 0.31730767, 0.18969086, 0.03979282]),\n",
       " array([0.60783145, 0.2905068 , 0.02805597, 0.01798188, 0.0556239 ]),\n",
       " array([0.68040449, 0.17741511, 0.09283212, 0.02766843, 0.02167985]),\n",
       " array([0.01249778, 0.01816936, 0.14632012, 0.26269199, 0.56032076]),\n",
       " array([0.76745004, 0.10437127, 0.0435137 , 0.02281137, 0.06185362]),\n",
       " array([0.11473389, 0.23084193, 0.28080704, 0.16050001, 0.21311714]),\n",
       " array([0.12733617, 0.13300272, 0.10718648, 0.20231483, 0.4301598 ]),\n",
       " array([0.08650749, 0.15925212, 0.15348814, 0.16532497, 0.43542729]),\n",
       " array([0.0190538 , 0.03603192, 0.24307832, 0.3937341 , 0.30810185]),\n",
       " array([0.04063259, 0.20670673, 0.55645233, 0.13256924, 0.06363911]),\n",
       " array([0.09477939, 0.58935442, 0.15799054, 0.13611618, 0.02175947]),\n",
       " array([0.02203313, 0.06102746, 0.12106716, 0.61196112, 0.18391113]),\n",
       " array([0.05454285, 0.12019133, 0.4767124 , 0.30151341, 0.04704001]),\n",
       " array([0.02261092, 0.04238707, 0.14074649, 0.39175431, 0.40250121]),\n",
       " array([0.21894647, 0.10129673, 0.15226872, 0.21843439, 0.30905368]),\n",
       " array([0.35323944, 0.47957143, 0.06856656, 0.02989268, 0.06872989]),\n",
       " array([0.06816289, 0.42060964, 0.30040507, 0.10428399, 0.1065384 ]),\n",
       " array([0.08275765, 0.01827443, 0.02338462, 0.23237177, 0.64321153]),\n",
       " array([0.04568623, 0.02475431, 0.038697  , 0.06014983, 0.83071262]),\n",
       " array([0.05092382, 0.08129552, 0.31924752, 0.36612426, 0.18240888]),\n",
       " array([0.51895507, 0.06972233, 0.18229405, 0.17080096, 0.05822759]),\n",
       " array([0.02134695, 0.06029419, 0.0930031 , 0.48405333, 0.34130243]),\n",
       " array([0.51905347, 0.31652353, 0.04665969, 0.04943395, 0.06832936]),\n",
       " array([0.0249475 , 0.29607816, 0.36930918, 0.2844857 , 0.02517946]),\n",
       " array([0.42040008, 0.18381465, 0.18713708, 0.08348997, 0.12515823]),\n",
       " array([0.56838472, 0.37375774, 0.0198581 , 0.01580098, 0.02219846]),\n",
       " array([0.75249515, 0.11728727, 0.07256243, 0.03185702, 0.02579813]),\n",
       " array([0.08018028, 0.05727252, 0.13846025, 0.40725624, 0.3168307 ]),\n",
       " array([0.01023402, 0.07611763, 0.25963748, 0.45939336, 0.1946175 ]),\n",
       " array([0.13966386, 0.1546868 , 0.20199344, 0.16818918, 0.33546672]),\n",
       " array([0.18795329, 0.64232732, 0.11912405, 0.02419236, 0.02640298]),\n",
       " array([0.58878913, 0.24448983, 0.10509251, 0.02752235, 0.03410618]),\n",
       " array([0.10909179, 0.16952097, 0.47764155, 0.17202871, 0.07171698]),\n",
       " array([0.55043987, 0.19438161, 0.05666274, 0.02854965, 0.16996613]),\n",
       " array([0.00229304, 0.00851989, 0.05172326, 0.42045164, 0.51701217]),\n",
       " array([0.60832361, 0.15135505, 0.04063574, 0.0382667 , 0.1614189 ]),\n",
       " array([0.7528695 , 0.06771819, 0.0360192 , 0.01660252, 0.1267906 ]),\n",
       " array([0.0986401 , 0.50056671, 0.23259064, 0.13742089, 0.03078166]),\n",
       " array([0.06458105, 0.18411048, 0.43285249, 0.23559314, 0.08286284]),\n",
       " array([0.57599229, 0.23618698, 0.09554776, 0.03903659, 0.05323639]),\n",
       " array([0.07877053, 0.20940161, 0.45272237, 0.19936341, 0.05974208]),\n",
       " array([0.12794086, 0.1959039 , 0.19559247, 0.33263789, 0.14792489]),\n",
       " array([0.10544101, 0.46804525, 0.33288124, 0.06485582, 0.02877668]),\n",
       " array([0.04197961, 0.39983049, 0.41858067, 0.12515933, 0.0144499 ]),\n",
       " array([0.03140061, 0.0460209 , 0.08955365, 0.39353847, 0.43948637]),\n",
       " array([0.0042256 , 0.01735857, 0.09319563, 0.2890604 , 0.5961598 ]),\n",
       " array([0.01393385, 0.21542097, 0.70793801, 0.02704957, 0.0356576 ]),\n",
       " array([0.12281854, 0.46207785, 0.3782689 , 0.02542443, 0.01141028]),\n",
       " array([0.00095033, 0.04794913, 0.52049445, 0.42732781, 0.00327829]),\n",
       " array([0.12613794, 0.46042859, 0.31526866, 0.06461349, 0.03355132]),\n",
       " array([0.03890037, 0.17025667, 0.5971099 , 0.15086176, 0.0428713 ]),\n",
       " array([0.07569762, 0.05542695, 0.28692876, 0.34035722, 0.24158945]),\n",
       " array([0.00902022, 0.02145508, 0.02162797, 0.29320105, 0.65469568]),\n",
       " array([0.04386133, 0.08126952, 0.22544635, 0.3410166 , 0.30840619]),\n",
       " array([0.01148546, 0.03749484, 0.07600409, 0.57492627, 0.30008933]),\n",
       " array([0.01835479, 0.0497822 , 0.17298299, 0.18981721, 0.56906282]),\n",
       " array([0.39505133, 0.20306964, 0.13475595, 0.17629239, 0.09083069]),\n",
       " array([0.00219599, 0.02144972, 0.29163017, 0.50738547, 0.17733865]),\n",
       " array([0.12451099, 0.49676931, 0.2333267 , 0.10545081, 0.03994219]),\n",
       " array([0.47493072, 0.31563059, 0.09209356, 0.04265262, 0.07469251]),\n",
       " array([0.22715093, 0.27984727, 0.38354527, 0.07013936, 0.03931717]),\n",
       " array([0.01254953, 0.03876291, 0.03455831, 0.3277117 , 0.58641756]),\n",
       " array([0.006938  , 0.04007768, 0.15321624, 0.61381496, 0.18595312]),\n",
       " array([0.05011216, 0.13088987, 0.33409416, 0.39357829, 0.09132552]),\n",
       " array([0.14480482, 0.70474638, 0.10741495, 0.02568806, 0.01734579]),\n",
       " array([0.00602386, 0.06532245, 0.24509017, 0.34899672, 0.33456679]),\n",
       " array([0.50638605, 0.26204397, 0.06757461, 0.09510376, 0.06889161]),\n",
       " array([0.70116103, 0.22825373, 0.01910424, 0.03804491, 0.01343609]),\n",
       " array([0.85502493, 0.02123814, 0.0105758 , 0.02061559, 0.09254554]),\n",
       " array([0.01668289, 0.11085644, 0.09348424, 0.48084954, 0.29812688]),\n",
       " array([0.02100665, 0.01834718, 0.02505375, 0.07554549, 0.86004693]),\n",
       " array([0.00729445, 0.15378838, 0.45899405, 0.35040896, 0.02951416]),\n",
       " array([0.01921189, 0.0636192 , 0.09152773, 0.45690747, 0.36873371]),\n",
       " array([0.03685661, 0.17101156, 0.39185925, 0.19588896, 0.20438362]),\n",
       " array([0.01617553, 0.10993884, 0.12555376, 0.45132564, 0.29700624]),\n",
       " array([0.35093119, 0.39887186, 0.08804497, 0.09740786, 0.06474413]),\n",
       " array([0.0178371 , 0.0350448 , 0.04816816, 0.31143482, 0.58751513]),\n",
       " array([0.01064042, 0.03823991, 0.08404905, 0.31261439, 0.55445622]),\n",
       " array([0.09609499, 0.43044167, 0.29023615, 0.15086053, 0.03236666]),\n",
       " array([0.42661349, 0.37138786, 0.08321399, 0.05808288, 0.06070178]),\n",
       " array([0.84498173, 0.07706299, 0.02694432, 0.01574651, 0.03526446]),\n",
       " array([0.33488776, 0.10689298, 0.20102032, 0.28230359, 0.07489534]),\n",
       " array([0.02794899, 0.1321379 , 0.31897463, 0.44945993, 0.07147855]),\n",
       " array([0.05086103, 0.08782774, 0.45883278, 0.37178661, 0.03069184]),\n",
       " array([0.0211592 , 0.01512681, 0.04010186, 0.59161001, 0.33200212]),\n",
       " array([0.01927823, 0.30703181, 0.23823715, 0.30943644, 0.12601638]),\n",
       " array([0.01409521, 0.07751918, 0.24618572, 0.50425015, 0.15794974]),\n",
       " array([0.03325186, 0.01752812, 0.04173357, 0.14880998, 0.75867648]),\n",
       " array([0.04235748, 0.02360862, 0.07909854, 0.25363942, 0.60129593]),\n",
       " array([0.18528815, 0.43473558, 0.24822222, 0.08540847, 0.04634559]),\n",
       " array([0.0166581 , 0.05245492, 0.0870348 , 0.52316513, 0.32068705]),\n",
       " array([0.05765883, 0.24968065, 0.43660183, 0.20134271, 0.05471598]),\n",
       " array([0.11250536, 0.2420064 , 0.27407692, 0.34422186, 0.02718945]),\n",
       " array([0.0181924 , 0.0837968 , 0.3476383 , 0.26727513, 0.28309738]),\n",
       " array([0.06714235, 0.35788712, 0.36454757, 0.15535069, 0.05507227]),\n",
       " array([0.04028293, 0.09127658, 0.1655694 , 0.54055073, 0.16232037]),\n",
       " array([0.62962154, 0.13606385, 0.09196441, 0.10295579, 0.0393944 ]),\n",
       " array([0.2633593 , 0.23614892, 0.22098388, 0.1624561 , 0.11705179]),\n",
       " array([0.26805963, 0.32149465, 0.16401971, 0.18593245, 0.06049356]),\n",
       " array([0.76990076, 0.16110366, 0.02687111, 0.00873789, 0.03338658]),\n",
       " array([0.29646717, 0.33096027, 0.32339243, 0.03472067, 0.01445946]),\n",
       " array([0.12032901, 0.25383973, 0.38294151, 0.15459042, 0.08829933]),\n",
       " array([0.02438411, 0.06638298, 0.05319337, 0.41460829, 0.44143125]),\n",
       " array([0.20097666, 0.18877718, 0.13568179, 0.18182747, 0.2927369 ]),\n",
       " array([0.14241365, 0.60230016, 0.1393071 , 0.06375737, 0.05222171]),\n",
       " array([0.01940672, 0.37383434, 0.37664665, 0.1505639 , 0.07954838]),\n",
       " array([0.05627953, 0.34696357, 0.33373469, 0.22729407, 0.03572814]),\n",
       " array([0.65407805, 0.24336927, 0.04320795, 0.02413736, 0.03520737]),\n",
       " array([0.28373156, 0.34110517, 0.1255466 , 0.16675379, 0.08286288]),\n",
       " array([0.23738123, 0.41204857, 0.22772334, 0.10137111, 0.02147575]),\n",
       " array([0.17111409, 0.33191474, 0.2789528 , 0.15257425, 0.06544412]),\n",
       " array([0.15797306, 0.31176474, 0.16929842, 0.15159806, 0.20936571]),\n",
       " array([0.19459563, 0.13526682, 0.22813678, 0.17602802, 0.26597275]),\n",
       " array([0.07315527, 0.16351978, 0.0970995 , 0.24699593, 0.41922952]),\n",
       " array([0.09288637, 0.27229494, 0.34961165, 0.25798205, 0.02722499]),\n",
       " array([0.01232179, 0.03207627, 0.07154756, 0.3895181 , 0.49453628]),\n",
       " array([0.03651552, 0.22467542, 0.33545045, 0.07090948, 0.33244913]),\n",
       " array([0.71234979, 0.14594939, 0.05658797, 0.06915347, 0.01595937]),\n",
       " array([0.09398418, 0.41260236, 0.19674541, 0.22245233, 0.07421572]),\n",
       " array([0.02135558, 0.02846426, 0.29647178, 0.3523679 , 0.30134048]),\n",
       " array([0.022116  , 0.0865531 , 0.30734806, 0.44393002, 0.14005282]),\n",
       " array([0.0040295 , 0.08514573, 0.49430591, 0.37766236, 0.03885651]),\n",
       " array([0.39658527, 0.34707066, 0.1651779 , 0.03704155, 0.05412461]),\n",
       " array([0.23650663, 0.41229617, 0.2682238 , 0.0555171 , 0.0274563 ]),\n",
       " array([0.12382057, 0.49320275, 0.24757418, 0.12445532, 0.01094718]),\n",
       " array([0.00314255, 0.01789393, 0.17585107, 0.43577463, 0.36733782]),\n",
       " array([0.44762521, 0.19555982, 0.11160168, 0.15161623, 0.09359705]),\n",
       " array([0.1628966 , 0.32600042, 0.15673974, 0.11058693, 0.24377631]),\n",
       " array([0.0491036 , 0.11881602, 0.25206656, 0.33124153, 0.2487723 ]),\n",
       " array([0.75587118, 0.08857446, 0.05453796, 0.01957234, 0.08144405]),\n",
       " array([0.15594936, 0.43685067, 0.29101829, 0.10814446, 0.00803721]),\n",
       " array([0.64142047, 0.26704875, 0.05829295, 0.01908574, 0.01415209]),\n",
       " array([0.70639421, 0.13378765, 0.08803719, 0.04682301, 0.02495794]),\n",
       " array([0.13570613, 0.45009019, 0.14904747, 0.14648614, 0.11867007]),\n",
       " array([0.02962857, 0.18810889, 0.50240014, 0.26066941, 0.01919299]),\n",
       " array([0.03234465, 0.43894855, 0.22159671, 0.22770007, 0.07941003]),\n",
       " array([0.02135199, 0.34435584, 0.38062091, 0.14538887, 0.10828238]),\n",
       " array([0.07744039, 0.21057906, 0.40219245, 0.25245221, 0.05733588]),\n",
       " array([0.01600018, 0.08702703, 0.13449614, 0.20700401, 0.55547264]),\n",
       " array([0.24072962, 0.52096111, 0.19035543, 0.02251351, 0.02544033]),\n",
       " array([0.0959978 , 0.29542843, 0.30830996, 0.24726338, 0.05300042]),\n",
       " array([0.11058941, 0.11460286, 0.14029467, 0.29046802, 0.34404504]),\n",
       " array([0.01816783, 0.0699125 , 0.09360223, 0.51426062, 0.30405683]),\n",
       " array([0.00454015, 0.0555316 , 0.46095297, 0.46741425, 0.01156103]),\n",
       " array([0.00801565, 0.03192436, 0.19029954, 0.62079585, 0.1489646 ]),\n",
       " array([0.12786265, 0.38026478, 0.11345489, 0.07275665, 0.30566102]),\n",
       " array([0.01917426, 0.08332286, 0.67504926, 0.21764733, 0.00480628]),\n",
       " array([0.08522228, 0.48210089, 0.23605665, 0.1479165 , 0.04870368]),\n",
       " array([0.54344379, 0.37449353, 0.06892501, 0.00869293, 0.00444474]),\n",
       " array([0.00658419, 0.03296734, 0.13964775, 0.52402934, 0.29677138]),\n",
       " array([0.07142007, 0.09046782, 0.23015231, 0.2681872 , 0.33977261]),\n",
       " array([0.10738843, 0.14711893, 0.08114631, 0.42717846, 0.23716787]),\n",
       " array([0.73649668, 0.10565036, 0.0598322 , 0.03786473, 0.06015604]),\n",
       " array([0.11059479, 0.14680173, 0.22423977, 0.38700259, 0.13136111]),\n",
       " array([0.02189012, 0.0646619 , 0.098223  , 0.44526926, 0.36995572]),\n",
       " array([0.05373742, 0.20404087, 0.1860503 , 0.22427125, 0.33190018]),\n",
       " array([0.06131119, 0.43980591, 0.37302773, 0.11088096, 0.01497421]),\n",
       " array([0.00375732, 0.00551036, 0.05519535, 0.303003  , 0.63253397]),\n",
       " array([0.06146632, 0.18176347, 0.26798635, 0.2812887 , 0.20749516]),\n",
       " array([0.04313855, 0.29795393, 0.42446313, 0.19699878, 0.03744562]),\n",
       " array([0.8786407 , 0.08206782, 0.00524753, 0.0049065 , 0.02913744]),\n",
       " array([0.06344128, 0.05899647, 0.16353561, 0.2528677 , 0.46115895]),\n",
       " array([0.00382572, 0.03717647, 0.11804137, 0.65978653, 0.18116992]),\n",
       " array([0.0737133 , 0.03547967, 0.09882699, 0.41859924, 0.37338081]),\n",
       " array([0.18812414, 0.4584245 , 0.25396386, 0.08223586, 0.01725164]),\n",
       " array([0.69075423, 0.14461235, 0.03232541, 0.02408325, 0.10822475]),\n",
       " array([0.01533368, 0.05749234, 0.08792042, 0.47907353, 0.36018003]),\n",
       " array([0.02333053, 0.17540321, 0.25248074, 0.20611929, 0.34266623]),\n",
       " array([0.01665518, 0.30656345, 0.31899894, 0.31999284, 0.03778959]),\n",
       " array([0.43852519, 0.06904523, 0.01568214, 0.07253511, 0.40421232]),\n",
       " array([0.26571068, 0.34205004, 0.17861186, 0.10691747, 0.10670995]),\n",
       " array([0.10977206, 0.11748278, 0.07776429, 0.40424655, 0.29073431]),\n",
       " array([0.01473367, 0.01948143, 0.04637283, 0.26960878, 0.64980329]),\n",
       " array([0.12351111, 0.43423706, 0.24551637, 0.16135709, 0.03537836]),\n",
       " array([0.26354909, 0.44035143, 0.17449324, 0.06761625, 0.05398999]),\n",
       " array([0.65948028, 0.24031712, 0.03999912, 0.03562619, 0.0245773 ]),\n",
       " array([0.03788141, 0.12464943, 0.05757537, 0.30148705, 0.47840674]),\n",
       " array([0.05320656, 0.04626418, 0.14669311, 0.22006063, 0.53377552]),\n",
       " array([0.45756831, 0.10441526, 0.04240217, 0.02729285, 0.3683214 ]),\n",
       " array([0.48352491, 0.11317441, 0.12417201, 0.12072761, 0.15840106]),\n",
       " array([0.24692576, 0.51993048, 0.17898512, 0.04768713, 0.00647151]),\n",
       " array([0.00689531, 0.02759299, 0.04827665, 0.31995147, 0.59728359]),\n",
       " array([0.00207853, 0.1779778 , 0.19603329, 0.52918654, 0.09472384]),\n",
       " array([0.01563665, 0.00838934, 0.02106006, 0.35669082, 0.59822314]),\n",
       " array([0.02660092, 0.13913186, 0.56145638, 0.24379223, 0.02901861]),\n",
       " array([0.47921665, 0.20161909, 0.16875094, 0.06864696, 0.08176636]),\n",
       " array([0.12232304, 0.33124763, 0.38116028, 0.05550253, 0.10976652]),\n",
       " array([0.54627952, 0.25395407, 0.12239521, 0.02651143, 0.05085978]),\n",
       " array([0.11336151, 0.22401925, 0.19164983, 0.23289691, 0.23807249]),\n",
       " array([0.09149933, 0.2449616 , 0.47551425, 0.15603138, 0.03199344]),\n",
       " array([0.7123888 , 0.11966476, 0.05248922, 0.02629399, 0.08916323]),\n",
       " array([0.00914947, 0.01588029, 0.05859652, 0.45657881, 0.45979491]),\n",
       " array([0.00968492, 0.02618003, 0.16776884, 0.51899407, 0.27737213]),\n",
       " array([0.03790362, 0.03906991, 0.13647227, 0.20094982, 0.58560438]),\n",
       " array([0.1512615 , 0.26847059, 0.18305428, 0.23148268, 0.16573094]),\n",
       " array([0.01538874, 0.12125444, 0.18486016, 0.60872906, 0.0697676 ]),\n",
       " array([0.06943362, 0.13940797, 0.28288993, 0.35975372, 0.14851476]),\n",
       " array([0.13121331, 0.14778057, 0.49451815, 0.14965306, 0.07683491]),\n",
       " array([0.02195715, 0.0814017 , 0.46271109, 0.30577636, 0.1281537 ]),\n",
       " array([0.04348507, 0.30134422, 0.46691835, 0.15933788, 0.02891447]),\n",
       " array([0.81824709, 0.16013977, 0.00706126, 0.00545534, 0.00909654]),\n",
       " array([0.13415339, 0.47431153, 0.21520952, 0.13898098, 0.03734457]),\n",
       " array([0.0897748 , 0.5203263 , 0.27400987, 0.10382369, 0.01206535]),\n",
       " array([0.0278806 , 0.03821504, 0.20515319, 0.22363302, 0.50511815]),\n",
       " array([0.08891922, 0.24520181, 0.07930673, 0.28448693, 0.30208531]),\n",
       " array([0.07497816, 0.20794386, 0.33486653, 0.26467415, 0.11753729]),\n",
       " array([0.45646694, 0.38716676, 0.05217411, 0.05302857, 0.05116362]),\n",
       " array([0.00505648, 0.01363588, 0.16778625, 0.57994006, 0.23358132]),\n",
       " array([0.0128765 , 0.03041589, 0.05971688, 0.05623378, 0.84075695]),\n",
       " array([0.20995458, 0.54486582, 0.22179807, 0.01448044, 0.00890109]),\n",
       " array([0.20393194, 0.33937978, 0.21540143, 0.19057887, 0.05070798]),\n",
       " array([0.49824041, 0.21832125, 0.05126522, 0.14358167, 0.08859146]),\n",
       " array([0.05001317, 0.03422392, 0.02593608, 0.14133167, 0.74849516]),\n",
       " array([0.11865152, 0.40387538, 0.33395519, 0.09337983, 0.05013808]),\n",
       " array([0.05358466, 0.10893615, 0.04964219, 0.18250393, 0.60533307]),\n",
       " array([0.03398556, 0.0943195 , 0.38241188, 0.41412966, 0.0751534 ]),\n",
       " array([0.49866906, 0.34508287, 0.07349015, 0.06560011, 0.01715781]),\n",
       " array([0.01071608, 0.03626831, 0.39893017, 0.50784219, 0.04624325]),\n",
       " array([0.15447688, 0.24321051, 0.28914957, 0.1367694 , 0.17639365]),\n",
       " array([0.02333507, 0.02327864, 0.0217826 , 0.23010101, 0.70150267]),\n",
       " array([0.00535394, 0.05314322, 0.57102666, 0.33864514, 0.03183104]),\n",
       " array([0.0110701 , 0.03222254, 0.16223006, 0.57392688, 0.22055042]),\n",
       " array([0.13393067, 0.13976292, 0.22001768, 0.32402459, 0.18226415]),\n",
       " array([0.00745442, 0.3796255 , 0.50139563, 0.10634983, 0.00517461]),\n",
       " array([0.52209498, 0.40174557, 0.02401894, 0.0214608 , 0.0306797 ]),\n",
       " array([0.90272664, 0.06044601, 0.01109183, 0.00824749, 0.01748802]),\n",
       " array([0.08795972, 0.52653131, 0.18304607, 0.1526724 , 0.0497905 ]),\n",
       " array([0.01298858, 0.09250838, 0.35219456, 0.42441517, 0.1178933 ]),\n",
       " array([0.0601537 , 0.02690182, 0.10149271, 0.25749136, 0.55396041]),\n",
       " array([0.24011412, 0.14330879, 0.32322743, 0.19673503, 0.09661465]),\n",
       " array([0.28506785, 0.10502705, 0.10315635, 0.13288757, 0.37386117]),\n",
       " array([0.63838189, 0.16075914, 0.10205847, 0.08654618, 0.01225432]),\n",
       " array([0.07119291, 0.28527664, 0.23222911, 0.35699652, 0.05430482]),\n",
       " array([0.00841255, 0.0171494 , 0.14626116, 0.38327216, 0.44490473]),\n",
       " array([0.184467  , 0.48841781, 0.21007313, 0.09861484, 0.01842721]),\n",
       " array([0.02018095, 0.17493538, 0.44035534, 0.3001476 , 0.06438073]),\n",
       " array([0.04836506, 0.49649622, 0.42108854, 0.02178991, 0.01226028]),\n",
       " array([0.00464741, 0.01666051, 0.08046909, 0.26522806, 0.63299492]),\n",
       " array([0.73978207, 0.11895746, 0.03813109, 0.02963454, 0.07349485]),\n",
       " array([0.04713579, 0.11556187, 0.33266546, 0.3254167 , 0.17922018]),\n",
       " array([0.23746888, 0.49339386, 0.19923219, 0.03666443, 0.03324064]),\n",
       " array([0.69819021, 0.19494034, 0.04109849, 0.03388461, 0.03188635]),\n",
       " array([0.82166546, 0.08860905, 0.04222406, 0.02501487, 0.02248656]),\n",
       " array([0.04833225, 0.18033506, 0.41698562, 0.29370932, 0.06063775]),\n",
       " array([0.3515661 , 0.32289121, 0.18201837, 0.06359971, 0.07992461]),\n",
       " array([0.1044399 , 0.29774279, 0.4746695 , 0.09656104, 0.02658677]),\n",
       " array([0.00695515, 0.25211251, 0.51950969, 0.20052035, 0.0209023 ]),\n",
       " array([0.13758305, 0.19579559, 0.10858594, 0.15202096, 0.40601446]),\n",
       " array([0.09025143, 0.03321121, 0.03562753, 0.35293134, 0.4879785 ]),\n",
       " array([0.04494584, 0.06952024, 0.18253249, 0.21677296, 0.48622847]),\n",
       " array([0.01222887, 0.03480882, 0.08531951, 0.47289289, 0.3947499 ]),\n",
       " array([0.34477632, 0.41519883, 0.09691792, 0.09515866, 0.04794827]),\n",
       " array([0.00860291, 0.02600284, 0.16126351, 0.34556826, 0.45856248]),\n",
       " array([0.08508642, 0.09589968, 0.08985656, 0.33573462, 0.39342272]),\n",
       " array([0.20165982, 0.03369805, 0.04246133, 0.16791986, 0.55426093]),\n",
       " array([0.05473354, 0.05878008, 0.1304848 , 0.2864161 , 0.46958548]),\n",
       " array([0.01892498, 0.02020737, 0.07683405, 0.19703571, 0.6869979 ]),\n",
       " array([0.00777992, 0.12334088, 0.43029799, 0.39418732, 0.0443939 ]),\n",
       " array([0.14565571, 0.59273758, 0.08321603, 0.06690051, 0.11149018]),\n",
       " array([0.01268059, 0.10617278, 0.51772259, 0.33092681, 0.03249723]),\n",
       " array([0.02487126, 0.15238521, 0.51008508, 0.16837708, 0.14428138]),\n",
       " array([0.02498475, 0.3105725 , 0.42658569, 0.1752763 , 0.06258076]),\n",
       " array([0.14292417, 0.13738675, 0.48801816, 0.17025885, 0.06141207]),\n",
       " array([0.74449456, 0.12923142, 0.0292919 , 0.02504722, 0.07193489]),\n",
       " array([0.00855519, 0.01176027, 0.02553989, 0.1891153 , 0.76502935]),\n",
       " array([0.1867528 , 0.53534321, 0.16805248, 0.02157256, 0.08827895]),\n",
       " array([0.02573053, 0.01497708, 0.03715961, 0.18095097, 0.74118181]),\n",
       " array([0.03241254, 0.43092607, 0.48834164, 0.0299247 , 0.01839504]),\n",
       " array([0.01361699, 0.02008199, 0.06420911, 0.26013378, 0.64195812]),\n",
       " array([0.07729307, 0.23215198, 0.10173301, 0.21669167, 0.37213026]),\n",
       " array([0.01857473, 0.13727223, 0.5313748 , 0.2952624 , 0.01751584]),\n",
       " array([0.20122656, 0.47748041, 0.19373151, 0.09067967, 0.03688185]),\n",
       " array([0.41351596, 0.08417813, 0.08782364, 0.05131609, 0.36316617]),\n",
       " array([0.01049438, 0.29278135, 0.48753571, 0.2039413 , 0.00524726]),\n",
       " array([0.20616309, 0.1123579 , 0.33058146, 0.19710096, 0.15379659]),\n",
       " array([0.04520377, 0.36753448, 0.33974398, 0.13562487, 0.1118929 ]),\n",
       " array([0.03694785, 0.15867777, 0.55167795, 0.17259824, 0.08009819]),\n",
       " array([0.2762013 , 0.19128894, 0.18396319, 0.11116091, 0.23738567]),\n",
       " ...]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appending the probabilities to a dataframe\n",
    "y_train_unlab_proba = list(y_train_unlab_proba)\n",
    "y_train_unlab_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nde0ejPEvvYC"
   },
   "outputs": [],
   "source": [
    "# Get the class labels\n",
    "# predict method returns the class label of the observation based on the highest class probability \n",
    "y_train_unlab =  log_model.predict(train_df_unlab.fillna(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhKbr6_xvvYE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appending the predictions to a dataframe\n",
    "y_train_unlab = list(y_train_unlab)\n",
    "y_train_unlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOvtxA_FvvYF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>had good experience when my wife and sat at th...</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130672</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "      <td>0.283654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348457</td>\n",
       "      <td>0.565034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>one of our favorite places to go when it is co...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.953704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>the doctor was very nice got in in good amount...</td>\n",
       "      <td>0.133654</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174229</td>\n",
       "      <td>0.616071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>the nook is an immediate phoenix staple came h...</td>\n",
       "      <td>0.188462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226860</td>\n",
       "      <td>0.549492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50214  had good experience when my wife and sat at th...    0.085577   \n",
       "50215  on my first to montreal with my gf we came her...    0.283654   \n",
       "50216  one of our favorite places to go when it is co...    0.025000   \n",
       "50217  the doctor was very nice got in in good amount...    0.133654   \n",
       "50218  the nook is an immediate phoenix staple came h...    0.188462   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50214  0.000000               0.000000                 0.0          0.130672   \n",
       "50215  0.000000               0.000000                 0.0          0.348457   \n",
       "50216  0.000000               0.006623                 0.0          0.045372   \n",
       "50217  0.009174               0.019868                 0.0          0.174229   \n",
       "50218  0.000000               0.006623                 0.0          0.226860   \n",
       "\n",
       "       words_vs_unique  \n",
       "ids                     \n",
       "50214         0.763889  \n",
       "50215         0.565034  \n",
       "50216         0.953704  \n",
       "50217         0.616071  \n",
       "50218         0.549492  "
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPOKZMDFvvYH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>label</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>had good experience when my wife and sat at th...</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130672</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3029679508072741, 0.13528450560284286, 0.28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "      <td>0.283654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348457</td>\n",
       "      <td>0.565034</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.006193083729534292, 0.12128443188432902, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>one of our favorite places to go when it is co...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.03860827433542501, 0.08587971408413816, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>the doctor was very nice got in in good amount...</td>\n",
       "      <td>0.133654</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174229</td>\n",
       "      <td>0.616071</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5880330922926519, 0.17142657793184435, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>the nook is an immediate phoenix staple came h...</td>\n",
       "      <td>0.188462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226860</td>\n",
       "      <td>0.549492</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.007379595186535904, 0.015515921839902561, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50214  had good experience when my wife and sat at th...    0.085577   \n",
       "50215  on my first to montreal with my gf we came her...    0.283654   \n",
       "50216  one of our favorite places to go when it is co...    0.025000   \n",
       "50217  the doctor was very nice got in in good amount...    0.133654   \n",
       "50218  the nook is an immediate phoenix staple came h...    0.188462   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50214  0.000000               0.000000                 0.0          0.130672   \n",
       "50215  0.000000               0.000000                 0.0          0.348457   \n",
       "50216  0.000000               0.006623                 0.0          0.045372   \n",
       "50217  0.009174               0.019868                 0.0          0.174229   \n",
       "50218  0.000000               0.006623                 0.0          0.226860   \n",
       "\n",
       "       words_vs_unique  label  \\\n",
       "ids                             \n",
       "50214         0.763889      1   \n",
       "50215         0.565034      4   \n",
       "50216         0.953704      5   \n",
       "50217         0.616071      1   \n",
       "50218         0.549492      5   \n",
       "\n",
       "                                                   proba  \n",
       "ids                                                       \n",
       "50214  [0.3029679508072741, 0.13528450560284286, 0.28...  \n",
       "50215  [0.006193083729534292, 0.12128443188432902, 0....  \n",
       "50216  [0.03860827433542501, 0.08587971408413816, 0.0...  \n",
       "50217  [0.5880330922926519, 0.17142657793184435, 0.09...  \n",
       "50218  [0.007379595186535904, 0.015515921839902561, 0...  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the label and the probability columns in our dataframe\n",
    "train_df_unlab['label'] = y_train_unlab\n",
    "train_df_unlab['proba'] = y_train_unlab_proba\n",
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, logistic regression model gives a probability values for each observation. \n",
    "We select those observations which have class probability values of more than 0.8. \n",
    "Doing so we ensure that we only add those pseudo labels for which the model is atleast 80% confident.\n",
    "We add those pseudo labelled rows from our unlabeled dataset to the labeled dataset as shown below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d3QTKGtvvYI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>label</th>\n",
       "      <th>proba</th>\n",
       "      <th>max_proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50214</th>\n",
       "      <td>had good experience when my wife and sat at th...</td>\n",
       "      <td>0.085577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130672</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3029679508072741, 0.13528450560284286, 0.28...</td>\n",
       "      <td>0.302968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50215</th>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "      <td>0.283654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348457</td>\n",
       "      <td>0.565034</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.006193083729534292, 0.12128443188432902, 0....</td>\n",
       "      <td>0.362747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>one of our favorite places to go when it is co...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045372</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.03860827433542501, 0.08587971408413816, 0.0...</td>\n",
       "      <td>0.626770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50217</th>\n",
       "      <td>the doctor was very nice got in in good amount...</td>\n",
       "      <td>0.133654</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174229</td>\n",
       "      <td>0.616071</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5880330922926519, 0.17142657793184435, 0.09...</td>\n",
       "      <td>0.588033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50218</th>\n",
       "      <td>the nook is an immediate phoenix staple came h...</td>\n",
       "      <td>0.188462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226860</td>\n",
       "      <td>0.549492</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.007379595186535904, 0.015515921839902561, 0...</td>\n",
       "      <td>0.704653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50214  had good experience when my wife and sat at th...    0.085577   \n",
       "50215  on my first to montreal with my gf we came her...    0.283654   \n",
       "50216  one of our favorite places to go when it is co...    0.025000   \n",
       "50217  the doctor was very nice got in in good amount...    0.133654   \n",
       "50218  the nook is an immediate phoenix staple came h...    0.188462   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50214  0.000000               0.000000                 0.0          0.130672   \n",
       "50215  0.000000               0.000000                 0.0          0.348457   \n",
       "50216  0.000000               0.006623                 0.0          0.045372   \n",
       "50217  0.009174               0.019868                 0.0          0.174229   \n",
       "50218  0.000000               0.006623                 0.0          0.226860   \n",
       "\n",
       "       words_vs_unique  label  \\\n",
       "ids                             \n",
       "50214         0.763889      1   \n",
       "50215         0.565034      4   \n",
       "50216         0.953704      5   \n",
       "50217         0.616071      1   \n",
       "50218         0.549492      5   \n",
       "\n",
       "                                                   proba  max_proba  \n",
       "ids                                                                  \n",
       "50214  [0.3029679508072741, 0.13528450560284286, 0.28...   0.302968  \n",
       "50215  [0.006193083729534292, 0.12128443188432902, 0....   0.362747  \n",
       "50216  [0.03860827433542501, 0.08587971408413816, 0.0...   0.626770  \n",
       "50217  [0.5880330922926519, 0.17142657793184435, 0.09...   0.588033  \n",
       "50218  [0.007379595186535904, 0.015515921839902561, 0...   0.704653  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_proba_list = []\n",
    "\n",
    "for rowlist in train_df_unlab['proba']:\n",
    "    max_proba_list.append(max(rowlist))\n",
    "    \n",
    "train_df_unlab['max_proba'] = max_proba_list\n",
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0VvcJrLvvYK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>label</th>\n",
       "      <th>proba</th>\n",
       "      <th>max_proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50293</th>\n",
       "      <td>do not get your policy with them worst custome...</td>\n",
       "      <td>0.070192</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114338</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8599410810652746, 0.0880713238120241, 0.035...</td>\n",
       "      <td>0.859941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50326</th>\n",
       "      <td>just love this place everyone there is so nice...</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.007364156786535241, 0.018323603818547887, 0...</td>\n",
       "      <td>0.813797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50354</th>\n",
       "      <td>all problems were small to negligible until go...</td>\n",
       "      <td>0.116346</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156080</td>\n",
       "      <td>0.641393</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8526026406729331, 0.07957033129108847, 0.03...</td>\n",
       "      <td>0.852603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50389</th>\n",
       "      <td>have used this company for few of my propertie...</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.594371</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8147498939757117, 0.06730999309230402, 0.05...</td>\n",
       "      <td>0.814750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50418</th>\n",
       "      <td>absolutely amazing food even better service th...</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.022481002779077424, 0.0153826785039694, 0.0...</td>\n",
       "      <td>0.908612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605056</th>\n",
       "      <td>this is by far the worst company my family and...</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.022936</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317604</td>\n",
       "      <td>0.516551</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8112203153354156, 0.0812188282745403, 0.028...</td>\n",
       "      <td>0.811220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605061</th>\n",
       "      <td>would not even give them NUM star this is the ...</td>\n",
       "      <td>0.090385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139746</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.870414921041628, 0.036086582936979125, 0.02...</td>\n",
       "      <td>0.870415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605073</th>\n",
       "      <td>if could give no stars would about week ago sc...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.284936</td>\n",
       "      <td>0.506705</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8358205461496548, 0.1090832884571438, 0.033...</td>\n",
       "      <td>0.835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605093</th>\n",
       "      <td>worse dme company ever NUM days before my due ...</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.059633</td>\n",
       "      <td>0.132450</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.442831</td>\n",
       "      <td>0.352854</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8226618840184522, 0.11424718645841289, 0.00...</td>\n",
       "      <td>0.822662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605107</th>\n",
       "      <td>worst mcdonalds have ever been to on multiple ...</td>\n",
       "      <td>0.017308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8118072749766416, 0.09630089575902186, 0.03...</td>\n",
       "      <td>0.811807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17519 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  word_count  \\\n",
       "ids                                                                     \n",
       "50293   do not get your policy with them worst custome...    0.070192   \n",
       "50326   just love this place everyone there is so nice...    0.037500   \n",
       "50354   all problems were small to negligible until go...    0.116346   \n",
       "50389   have used this company for few of my propertie...    0.144231   \n",
       "50418   absolutely amazing food even better service th...    0.015385   \n",
       "...                                                   ...         ...   \n",
       "605056  this is by far the worst company my family and...    0.275000   \n",
       "605061  would not even give them NUM star this is the ...    0.090385   \n",
       "605073  if could give no stars would about week ago sc...    0.250000   \n",
       "605093  worse dme company ever NUM days before my due ...    0.487500   \n",
       "605107  worst mcdonalds have ever been to on multiple ...    0.017308   \n",
       "\n",
       "        capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                             \n",
       "50293   0.032110               0.000000            0.000000          0.114338   \n",
       "50326   0.000000               0.013245            0.000000          0.052632   \n",
       "50354   0.009174               0.019868            0.000000          0.156080   \n",
       "50389   0.004587               0.006623            0.000000          0.183303   \n",
       "50418   0.004587               0.006623            0.000000          0.027223   \n",
       "...          ...                    ...                 ...               ...   \n",
       "605056  0.022936               0.006623            0.000000          0.317604   \n",
       "605061  0.000000               0.013245            0.000000          0.139746   \n",
       "605073  0.045872               0.006623            0.015385          0.284936   \n",
       "605093  0.059633               0.132450            0.092308          0.442831   \n",
       "605107  0.000000               0.000000            0.000000          0.032668   \n",
       "\n",
       "        words_vs_unique  label  \\\n",
       "ids                              \n",
       "50293          0.831081      1   \n",
       "50326          0.687500      5   \n",
       "50354          0.641393      1   \n",
       "50389          0.594371      1   \n",
       "50418          0.926471      5   \n",
       "...                 ...    ...   \n",
       "605056         0.516551      1   \n",
       "605061         0.776316      1   \n",
       "605073         0.506705      1   \n",
       "605093         0.352854      1   \n",
       "605107         1.000000      1   \n",
       "\n",
       "                                                    proba  max_proba  \n",
       "ids                                                                   \n",
       "50293   [0.8599410810652746, 0.0880713238120241, 0.035...   0.859941  \n",
       "50326   [0.007364156786535241, 0.018323603818547887, 0...   0.813797  \n",
       "50354   [0.8526026406729331, 0.07957033129108847, 0.03...   0.852603  \n",
       "50389   [0.8147498939757117, 0.06730999309230402, 0.05...   0.814750  \n",
       "50418   [0.022481002779077424, 0.0153826785039694, 0.0...   0.908612  \n",
       "...                                                   ...        ...  \n",
       "605056  [0.8112203153354156, 0.0812188282745403, 0.028...   0.811220  \n",
       "605061  [0.870414921041628, 0.036086582936979125, 0.02...   0.870415  \n",
       "605073  [0.8358205461496548, 0.1090832884571438, 0.033...   0.835821  \n",
       "605093  [0.8226618840184522, 0.11424718645841289, 0.00...   0.822662  \n",
       "605107  [0.8118072749766416, 0.09630089575902186, 0.03...   0.811807  \n",
       "\n",
       "[17519 rows x 10 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking rows having probability values more than 0.80 (high confidence rows) \n",
    "#This threshold indicates how certain the model is\n",
    "train_df_unlab = train_df_unlab[train_df_unlab.max_proba > 0.80]\n",
    "train_df_unlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bU26NEwlvvYL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50293</th>\n",
       "      <td>do not get your policy with them worst custome...</td>\n",
       "      <td>0.070192</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114338</td>\n",
       "      <td>0.831081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50326</th>\n",
       "      <td>just love this place everyone there is so nice...</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50354</th>\n",
       "      <td>all problems were small to negligible until go...</td>\n",
       "      <td>0.116346</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156080</td>\n",
       "      <td>0.641393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50389</th>\n",
       "      <td>have used this company for few of my propertie...</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.594371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50418</th>\n",
       "      <td>absolutely amazing food even better service th...</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "ids                                                                    \n",
       "50293  do not get your policy with them worst custome...    0.070192   \n",
       "50326  just love this place everyone there is so nice...    0.037500   \n",
       "50354  all problems were small to negligible until go...    0.116346   \n",
       "50389  have used this company for few of my propertie...    0.144231   \n",
       "50418  absolutely amazing food even better service th...    0.015385   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "ids                                                                            \n",
       "50293  0.032110               0.000000                 0.0          0.114338   \n",
       "50326  0.000000               0.013245                 0.0          0.052632   \n",
       "50354  0.009174               0.019868                 0.0          0.156080   \n",
       "50389  0.004587               0.006623                 0.0          0.183303   \n",
       "50418  0.004587               0.006623                 0.0          0.027223   \n",
       "\n",
       "       words_vs_unique  label  \n",
       "ids                            \n",
       "50293         0.831081      1  \n",
       "50326         0.687500      5  \n",
       "50354         0.641393      1  \n",
       "50389         0.594371      1  \n",
       "50418         0.926471      5  "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping maximum probability column and probability vector column\n",
    "train_df_unlab = train_df_unlab.drop(columns=['max_proba', 'proba'], axis=1)\n",
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ids                        int64\n",
       "text                      object\n",
       "label                      int64\n",
       "word_count               float64\n",
       "capitals                 float64\n",
       "num_exclamation_marks    float64\n",
       "num_question_marks       float64\n",
       "num_unique_words         float64\n",
       "words_vs_unique          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Min-Max Normalization \n",
    "for colname in textcountscols:\n",
    "    train_df[colname] = pd.to_numeric(train_df[colname])\n",
    "    train_df[colname] = (train_df[colname]-train_df[colname].min())/(train_df[colname].max()-\\\n",
    "                                                                           train_df[colname].min())\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_unlab.reset_index('ids', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50293</td>\n",
       "      <td>do not get your policy with them worst custome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070192</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114338</td>\n",
       "      <td>0.831081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50326</td>\n",
       "      <td>just love this place everyone there is so nice...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50354</td>\n",
       "      <td>all problems were small to negligible until go...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116346</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.019868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156080</td>\n",
       "      <td>0.641393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50389</td>\n",
       "      <td>have used this company for few of my propertie...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.594371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50418</td>\n",
       "      <td>absolutely amazing food even better service th...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027223</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ids                                               text  label  \\\n",
       "0  50293  do not get your policy with them worst custome...      1   \n",
       "1  50326  just love this place everyone there is so nice...      5   \n",
       "2  50354  all problems were small to negligible until go...      1   \n",
       "3  50389  have used this company for few of my propertie...      1   \n",
       "4  50418  absolutely amazing food even better service th...      5   \n",
       "\n",
       "   word_count  capitals  num_exclamation_marks  num_question_marks  \\\n",
       "0    0.070192  0.032110               0.000000                 0.0   \n",
       "1    0.037500  0.000000               0.013245                 0.0   \n",
       "2    0.116346  0.009174               0.019868                 0.0   \n",
       "3    0.144231  0.004587               0.006623                 0.0   \n",
       "4    0.015385  0.004587               0.006623                 0.0   \n",
       "\n",
       "   num_unique_words  words_vs_unique  \n",
       "0          0.114338         0.831081  \n",
       "1          0.052632         0.687500  \n",
       "2          0.156080         0.641393  \n",
       "3          0.183303         0.594371  \n",
       "4          0.027223         0.926471  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unlab= train_df_unlab[['ids','text','label','word_count','capitals','num_exclamation_marks','num_question_marks','num_unique_words','words_vs_unique']]\n",
    "train_df_unlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwyWrPXvvvYR"
   },
   "outputs": [],
   "source": [
    "#merging both the dataframes\n",
    "merged_df = pd.concat([train_df,train_df_unlab], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RB_pGR1AvvYT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the new rule is if you are waiting for table w...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.158582</td>\n",
       "      <td>0.643544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flirted with giving this two stars but that is...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.200784</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266791</td>\n",
       "      <td>0.523664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was staying at planet hollywood across the str...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>0.543736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food is good but prices are super expensive NU...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.126347</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.180970</td>\n",
       "      <td>0.610421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.120470</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>0.425646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  word_count  \\\n",
       "0  the new rule is if you are waiting for table w...      4    0.107738   \n",
       "1  flirted with giving this two stars but that is...      3    0.200784   \n",
       "2  was staying at planet hollywood across the str...      5    0.107738   \n",
       "3  food is good but prices are super expensive NU...      2    0.126347   \n",
       "4  worse company to deal with they do horrible wo...      1    0.120470   \n",
       "\n",
       "   capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0      0.02               0.000000            0.027027          0.158582   \n",
       "1      0.00               0.000000            0.000000          0.266791   \n",
       "2      0.00               0.011765            0.000000          0.145522   \n",
       "3      0.00               0.000000            0.027027          0.180970   \n",
       "4      0.00               0.000000            0.000000          0.145522   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.643544  \n",
       "1         0.523664  \n",
       "2         0.543736  \n",
       "3         0.610421  \n",
       "4         0.425646  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.drop(columns=['ids'],axis = 1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQVqKQ6OvvYZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67519, 8)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this manner, the high confidence rows from unlabeled dataset has been added to the labeled dataset and now we rebuild the logistic regression, multinomial naive bayes and ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UslIEzVivvYb"
   },
   "outputs": [],
   "source": [
    "#train test split in the ratio of 80:20\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_df.drop('label', axis=1), merged_df.label, \\\n",
    "                                                    test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23486</th>\n",
       "      <td>stayed at the rio this weekend and will be my ...</td>\n",
       "      <td>0.324192</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363806</td>\n",
       "      <td>0.351680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33911</th>\n",
       "      <td>exactly what needed when wanted quick snack lo...</td>\n",
       "      <td>0.042116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072761</td>\n",
       "      <td>0.856121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41958</th>\n",
       "      <td>have been here couple times now and have had m...</td>\n",
       "      <td>0.064643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>0.716538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10319</th>\n",
       "      <td>ate there once and short of being annoyed by a...</td>\n",
       "      <td>0.068560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110075</td>\n",
       "      <td>0.754798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21177</th>\n",
       "      <td>ate here last week initially enjoyed the ambia...</td>\n",
       "      <td>0.273262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296642</td>\n",
       "      <td>0.321715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>we loved our stay at the venetian wish we coul...</td>\n",
       "      <td>0.358472</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.560131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47508</th>\n",
       "      <td>not bad but have had better shumai their taro ...</td>\n",
       "      <td>0.024486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25015</th>\n",
       "      <td>had happy hour for the day after my wedding wh...</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093284</td>\n",
       "      <td>0.740547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9818</th>\n",
       "      <td>this is the only place that know of that can g...</td>\n",
       "      <td>0.123408</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173507</td>\n",
       "      <td>0.588756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64566</th>\n",
       "      <td>went into this location to make deposit the te...</td>\n",
       "      <td>0.214423</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212341</td>\n",
       "      <td>0.408482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54015 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "23486  stayed at the rio this weekend and will be my ...    0.324192   \n",
       "33911  exactly what needed when wanted quick snack lo...    0.042116   \n",
       "41958  have been here couple times now and have had m...    0.064643   \n",
       "10319  ate there once and short of being annoyed by a...    0.068560   \n",
       "21177  ate here last week initially enjoyed the ambia...    0.273262   \n",
       "...                                                  ...         ...   \n",
       "11825  we loved our stay at the venetian wish we coul...    0.358472   \n",
       "47508  not bad but have had better shumai their taro ...    0.024486   \n",
       "25015  had happy hour for the day after my wedding wh...    0.058766   \n",
       "9818   this is the only place that know of that can g...    0.123408   \n",
       "64566  went into this location to make deposit the te...    0.214423   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "23486  0.020000               0.000000            0.000000          0.363806   \n",
       "33911  0.000000               0.000000            0.000000          0.072761   \n",
       "41958  0.000000               0.011765            0.000000          0.100746   \n",
       "10319  0.000000               0.000000            0.000000          0.110075   \n",
       "21177  0.000000               0.000000            0.000000          0.296642   \n",
       "...         ...                    ...                 ...               ...   \n",
       "11825  0.020000               0.176471            0.054054          0.492537   \n",
       "47508  0.000000               0.000000            0.000000          0.046642   \n",
       "25015  0.000000               0.000000            0.000000          0.093284   \n",
       "9818   0.010000               0.000000            0.000000          0.173507   \n",
       "64566  0.004587               0.000000            0.000000          0.212341   \n",
       "\n",
       "       words_vs_unique  \n",
       "23486         0.351680  \n",
       "33911         0.856121  \n",
       "41958         0.716538  \n",
       "10319         0.754798  \n",
       "21177         0.321715  \n",
       "...                ...  \n",
       "11825         0.560131  \n",
       "47508         1.000000  \n",
       "25015         0.740547  \n",
       "9818          0.588756  \n",
       "64566         0.408482  \n",
       "\n",
       "[54015 rows x 7 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23486    1\n",
       "33911    4\n",
       "41958    4\n",
       "10319    2\n",
       "21177    1\n",
       "        ..\n",
       "11825    5\n",
       "47508    3\n",
       "25015    5\n",
       "9818     3\n",
       "64566    1\n",
       "Name: label, Length: 54015, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7dlO840vvYh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('textcounts',\n",
       "                                                 ColumnExtractor(cols=Index(['word_count', 'capitals', 'num_exclamation_marks', 'num_question_marks',\n",
       "       'num_unique_words', 'words_vs_unique'],\n",
       "      dtype='object'))),\n",
       "                                                ('pipe',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('cleantext',\n",
       "                                                                  ColumnExtractor(cols='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectoriz...\n",
       "                                                          verbose=False))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('log',\n",
       "                 OneVsRestClassifier(estimator=LogisticRegression(C=1.01,\n",
       "                                                                  class_weight=None,\n",
       "                                                                  dual=False,\n",
       "                                                                  fit_intercept=True,\n",
       "                                                                  intercept_scaling=1,\n",
       "                                                                  l1_ratio=None,\n",
       "                                                                  max_iter=100,\n",
       "                                                                  multi_class='warn',\n",
       "                                                                  n_jobs=None,\n",
       "                                                                  penalty='l2',\n",
       "                                                                  random_state=0,\n",
       "                                                                  solver='lbfgs',\n",
       "                                                                  tol=0.0001,\n",
       "                                                                  verbose=0,\n",
       "                                                                  warm_start=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#running logistic model on pseudo labels + train labels\n",
    "log_model.fit(X_train.fillna(' '), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbM8GDP2vvYi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7133441943127962\n"
     ]
    }
   ],
   "source": [
    "y_log_pred = log_model.predict(X_test.fillna(' '))\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_log_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy shown by logistic regression model on labelled+unlabelled dataset is 71.33**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy shown by Multinomial Naive Bayes was 68.60 on labelled+unlabelled dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy shown by Ensemble Model on labelled+unlabelled dataset was 70.93**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy shown by Stacked Model on labelled+unlabelled dataset was 71.05**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-oun1y9vvYv"
   },
   "source": [
    "## Preprocessing test data and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform predictions on the test data using logistic regression model and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mfH7UdU7vvYx",
    "outputId": "4920ab36-f485-4714-ada5-107e77ad7f8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>trying to have a nice quiet dinner.  the annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Been getting food to go from here for over 3yr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ugh. I've had to eat here a couple of times be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>The people here are so nice! I ordered on eat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>Heard alot of good things about this place and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                               text\n",
       "0  test_1  trying to have a nice quiet dinner.  the annou...\n",
       "1  test_2  Been getting food to go from here for over 3yr...\n",
       "2  test_3  Ugh. I've had to eat here a couple of times be...\n",
       "3  test_4  The people here are so nice! I ordered on eat ...\n",
       "4  test_5  Heard alot of good things about this place and..."
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the test file\n",
    "kaggle=pd.read_csv('test_data.csv')\n",
    "kaggle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the test dataset to the preprocessing function and we also generate numerical features as we did for the labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#generating numerical features\n",
    "kaggle=extract_features(kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document token pairing by making a dictionary where the keys are the doc ids and the values are\n",
    "# the tokens in the document\n",
    "tokenized_reuters =  dict(CleanData(row_id,row['text']) for row_id,row in kaggle.iterrows())\n",
    "\n",
    "\n",
    "# list of all words in the dataset\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
    "\n",
    "# list of words not in stop words\n",
    "words_imp = [w for w in words if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "0k3QzYgEvvY1",
    "outputId": "1d7584da-12c4-435d-8b06-7f6fb528def6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>trying to have a nice quiet dinner.  the annou...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Been getting food to go from here for over 3yr...</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ugh. I've had to eat here a couple of times be...</td>\n",
       "      <td>248</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.693548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>The people here are so nice! I ordered on eat ...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>Heard alot of good things about this place and...</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.831579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                               text  word_count  \\\n",
       "0  test_1  trying to have a nice quiet dinner.  the annou...          20   \n",
       "1  test_2  Been getting food to go from here for over 3yr...         124   \n",
       "2  test_3  Ugh. I've had to eat here a couple of times be...         248   \n",
       "3  test_4  The people here are so nice! I ordered on eat ...          32   \n",
       "4  test_5  Heard alot of good things about this place and...          95   \n",
       "\n",
       "   capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0         0                      0                   0                18   \n",
       "1         0                      0                   0                98   \n",
       "2         7                      3                   0               172   \n",
       "3         0                      3                   0                31   \n",
       "4         0                      2                   0                79   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.900000  \n",
       "1         0.790323  \n",
       "2         0.693548  \n",
       "3         0.968750  \n",
       "4         0.831579  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeping the best features\n",
    "drop_list = ['char_count', 'word_density', 'punc_count', 'total_length',\n",
    "       'caps_vs_length']\n",
    "kaggle.drop(columns=drop_list, axis = 1, inplace=True)\n",
    "kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oX5DPbEYvvY2"
   },
   "outputs": [],
   "source": [
    "#processing bigrams\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(words_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFC98MkevvY3"
   },
   "outputs": [],
   "source": [
    "best_bigrams=bigramFinder.nbest(bigrams.pmi, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZy5AK3ovvY4"
   },
   "outputs": [],
   "source": [
    "# Choose the relevant bigrams which have the POS tags we desire\n",
    "# Add them to a tagged list\n",
    "pos_tagged = list()\n",
    "for bigram in best_bigrams:\n",
    "    tagged_sentence = nltk.tag.pos_tag([bigram[0].strip('\\''),bigram[1].strip('\\'')])\n",
    "    \n",
    "    if tagged_sentence[0][1] in POS_TAGS or tagged_sentence[1][1] in POS_TAGS:\n",
    "        pos_tagged.append((tagged_sentence[0][0],tagged_sentence[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_fDvG2VvvY6"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the multi-word entities\n",
    "mwetokens = MWETokenizer(pos_tagged)\n",
    "\n",
    "colloc_units = dict()\n",
    "for key, value in tokenized_reuters.items():\n",
    "    colloc_units[key] = mwetokens.tokenize(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "58bivEHFvvY7",
    "outputId": "c8a85a66-b13a-4ebf-ce3d-450d8817c435"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['trying', 'to', 'have', 'nice', 'quiet', 'din...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['been', 'getting', 'food', 'to', 'go', 'from'...</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['ugh', 'have', 'had', 'to', 'eat', 'here', 'c...</td>\n",
       "      <td>248</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.693548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['the', 'people', 'here', 'are', 'so', 'nice',...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['heard', 'alot', 'of', 'good', 'things', 'abo...</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.831579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text  word_count  \\\n",
       "0    0  ['trying', 'to', 'have', 'nice', 'quiet', 'din...          20   \n",
       "1    1  ['been', 'getting', 'food', 'to', 'go', 'from'...         124   \n",
       "2    2  ['ugh', 'have', 'had', 'to', 'eat', 'here', 'c...         248   \n",
       "3    3  ['the', 'people', 'here', 'are', 'so', 'nice',...          32   \n",
       "4    4  ['heard', 'alot', 'of', 'good', 'things', 'abo...          95   \n",
       "\n",
       "   capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0         0                      0                   0                18   \n",
       "1         0                      0                   0                98   \n",
       "2         7                      3                   0               172   \n",
       "3         0                      3                   0                31   \n",
       "4         0                      2                   0                79   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.900000  \n",
       "1         0.790323  \n",
       "2         0.693548  \n",
       "3         0.968750  \n",
       "4         0.831579  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appending numerical features\n",
    "ids = [i for i in range(0,kaggle.shape[0])]\n",
    "kaggle['ids'] = ids\n",
    "train_df_kaggle=pd.DataFrame(colloc_units.items(), columns=['ids','text'])\n",
    "train_df_kaggle['text']=train_df_kaggle['text'].astype(str)\n",
    "train_df_kaggle['word_count'] = kaggle['word_count']\n",
    "train_df_kaggle['capitals'] = kaggle['capitals']\n",
    "train_df_kaggle['num_exclamation_marks'] = kaggle['num_exclamation_marks']\n",
    "train_df_kaggle['num_question_marks'] = kaggle['num_question_marks']\n",
    "train_df_kaggle['num_unique_words'] = kaggle['num_unique_words']\n",
    "train_df_kaggle['words_vs_unique'] = kaggle['words_vs_unique']\n",
    "train_df_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sr6-Ln2RvvY8",
    "outputId": "ccedaf70-1765-4305-9a03-095496024949"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['trying', 'to', 'have', 'nice', 'quiet', 'din...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['been', 'getting', 'food', 'to', 'go', 'from'...</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ugh', 'have', 'had', 'to', 'eat', 'here', 'c...</td>\n",
       "      <td>248</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.693548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['the', 'people', 'here', 'are', 'so', 'nice',...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['heard', 'alot', 'of', 'good', 'things', 'abo...</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0.831579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count  capitals  \\\n",
       "0  ['trying', 'to', 'have', 'nice', 'quiet', 'din...          20         0   \n",
       "1  ['been', 'getting', 'food', 'to', 'go', 'from'...         124         0   \n",
       "2  ['ugh', 'have', 'had', 'to', 'eat', 'here', 'c...         248         7   \n",
       "3  ['the', 'people', 'here', 'are', 'so', 'nice',...          32         0   \n",
       "4  ['heard', 'alot', 'of', 'good', 'things', 'abo...          95         0   \n",
       "\n",
       "   num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0                      0                   0                18   \n",
       "1                      0                   0                98   \n",
       "2                      3                   0               172   \n",
       "3                      3                   0                31   \n",
       "4                      2                   0                79   \n",
       "\n",
       "   words_vs_unique  \n",
       "0         0.900000  \n",
       "1         0.790323  \n",
       "2         0.693548  \n",
       "3         0.968750  \n",
       "4         0.831579  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_kaggle.drop('ids',axis=1,inplace=True)\n",
    "train_df_kaggle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3ijbXaZvvZB"
   },
   "outputs": [],
   "source": [
    "#merging the tokens\n",
    "train_df_kaggle['text'] = train_df_kaggle['text'].apply(merge_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "D2H2jVajvvZF",
    "outputId": "8c7b4e90-e811-4078-84f7-6e6bc4131c26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      object\n",
       "word_count               float64\n",
       "capitals                 float64\n",
       "num_exclamation_marks    float64\n",
       "num_question_marks       float64\n",
       "num_unique_words         float64\n",
       "words_vs_unique          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Min-Max Normalization\n",
    "for colname in textcountscols:\n",
    "    train_df_kaggle[colname] = pd.to_numeric(train_df_kaggle[colname])\n",
    "    train_df_kaggle[colname] = (train_df_kaggle[colname]-train_df_kaggle[colname].min())/(train_df_kaggle[colname].max()-\\\n",
    "                                                                                          train_df_kaggle[colname].min())\n",
    "train_df_kaggle.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>capitals</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trying to have nice quiet dinner the announcer...</td>\n",
       "      <td>0.018627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032015</td>\n",
       "      <td>0.851166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>been getting food to go from here for over NUM...</td>\n",
       "      <td>0.120588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182674</td>\n",
       "      <td>0.687929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ugh have had to eat here couple of times becau...</td>\n",
       "      <td>0.242157</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.543896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the people here are so nice ordered on eat NUM...</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.953489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>heard alot of good things about this place and...</td>\n",
       "      <td>0.092157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146893</td>\n",
       "      <td>0.749333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>their pizza is alright still think spinato is ...</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.797045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>are here for an event and it was as expected f...</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.864697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>came here for business lunch and was not impre...</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096045</td>\n",
       "      <td>0.601712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>have been working with charles over the last N...</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075330</td>\n",
       "      <td>0.757006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>at our second visit we were dismayed to find o...</td>\n",
       "      <td>0.142157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.201507</td>\n",
       "      <td>0.612624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  word_count  \\\n",
       "0      trying to have nice quiet dinner the announcer...    0.018627   \n",
       "1      been getting food to go from here for over NUM...    0.120588   \n",
       "2      ugh have had to eat here couple of times becau...    0.242157   \n",
       "3      the people here are so nice ordered on eat NUM...    0.030392   \n",
       "4      heard alot of good things about this place and...    0.092157   \n",
       "...                                                  ...         ...   \n",
       "49995  their pizza is alright still think spinato is ...    0.020588   \n",
       "49996  are here for an event and it was as expected f...    0.020588   \n",
       "49997  came here for business lunch and was not impre...    0.068627   \n",
       "49998  have been working with charles over the last N...    0.047059   \n",
       "49999  at our second visit we were dismayed to find o...    0.142157   \n",
       "\n",
       "       capitals  num_exclamation_marks  num_question_marks  num_unique_words  \\\n",
       "0      0.000000               0.000000            0.000000          0.032015   \n",
       "1      0.000000               0.000000            0.000000          0.182674   \n",
       "2      0.078652               0.023256            0.000000          0.322034   \n",
       "3      0.000000               0.023256            0.000000          0.056497   \n",
       "4      0.000000               0.015504            0.000000          0.146893   \n",
       "...         ...                    ...                 ...               ...   \n",
       "49995  0.000000               0.007752            0.000000          0.033898   \n",
       "49996  0.000000               0.000000            0.000000          0.035782   \n",
       "49997  0.011236               0.000000            0.000000          0.096045   \n",
       "49998  0.000000               0.000000            0.000000          0.075330   \n",
       "49999  0.000000               0.000000            0.026316          0.201507   \n",
       "\n",
       "       words_vs_unique  \n",
       "0             0.851166  \n",
       "1             0.687929  \n",
       "2             0.543896  \n",
       "3             0.953489  \n",
       "4             0.749333  \n",
       "...                ...  \n",
       "49995         0.797045  \n",
       "49996         0.864697  \n",
       "49997         0.601712  \n",
       "49998         0.757006  \n",
       "49999         0.612624  \n",
       "\n",
       "[50000 rows x 7 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y8FILPjvvZG"
   },
   "outputs": [],
   "source": [
    "#performing predictions\n",
    "kaggle_log_pred = log_model.predict(train_df_kaggle.fillna(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQQRmivWvvZH"
   },
   "outputs": [],
   "source": [
    "kaggle['label'] = kaggle_log_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Uzy8vVJbvvZL",
    "outputId": "c0fe79a8-b6c6-499f-e2d4-75f92453f14a"
   },
   "outputs": [],
   "source": [
    "kaggle.drop(columns=['text','word_count','capitals','num_exclamation_marks','num_question_marks','num_unique_words','words_vs_unique','ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yTjSy5MbvvZM",
    "outputId": "9984d550-5f45-4cb2-a199-b0f05b2b7a25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.to_csv('predict_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "* https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "* https://towardsdatascience.com/sentiment-analysis-simplified-ac30720a5827\n",
    "* https://medium.com/@robert.salgado/multiclass-text-classification-from-start-to-finish-f616a8642538\n",
    "* https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n",
    "* https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "* https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a\n",
    "* https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ADA_Ass_2_FeatureEngineering (3).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
